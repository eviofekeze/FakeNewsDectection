{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWwaZI_GE4cr"
      },
      "source": [
        "## Detecting Fake News and Real News\n",
        "\n",
        "**Contributors:** Clayton Fields, Evi Ofekeze, Justin Carpenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d8TgA-75E4cv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import textstat as ts\n",
        "import xgboost as xg\n",
        "from sklearn import svm\n",
        "import csv\n",
        "\n",
        "import scipy.sparse\n",
        "import scipy.sparse.csgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2mAufzXaxtwk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, make_scorer, roc_auc_score, average_precision_score, precision_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSvxpMKYE4c6"
      },
      "source": [
        "### Politifact: Load and Format News Content Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi15DxChE4c8",
        "outputId": "6ae051bf-a48e-48ee-97c1-37f64f433e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 120 entries, 0 to 0\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   top_img         118 non-null    object\n",
            " 1   text            120 non-null    object\n",
            " 2   authors         118 non-null    object\n",
            " 3   keywords        118 non-null    object\n",
            " 4   meta_data       118 non-null    object\n",
            " 5   canonical_link  118 non-null    object\n",
            " 6   images          118 non-null    object\n",
            " 7   title           120 non-null    object\n",
            " 8   url             118 non-null    object\n",
            " 9   summary         118 non-null    object\n",
            " 10  movies          118 non-null    object\n",
            " 11  publish_date    81 non-null     object\n",
            " 12  source          118 non-null    object\n",
            " 13  Real            120 non-null    int64 \n",
            "dtypes: int64(1), object(13)\n",
            "memory usage: 14.1+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#get real news content data\n",
        "temp = pd.read_json('PolitiFact/RealNewsContent/PolitiFact_Real_2-Webpage.json',orient='index')\n",
        "pf_real_news =  pd.DataFrame(columns=temp.index)\n",
        "for i in range(1,121):\n",
        "    path='PolitiFact/RealNewsContent/PolitiFact_Real_'+str(i)+'-Webpage.json'\n",
        "    df = pd.read_json(path,orient='index')\n",
        "    df = df.transpose()\n",
        "    pf_real_news = pd.concat([pf_real_news, df])\n",
        "del(df,path,i)\n",
        "\n",
        "\n",
        "#get fake news content data\n",
        "pf_fake_news =  pd.DataFrame(columns=temp.index)\n",
        "for i in range(1,121):\n",
        "    path='PolitiFact/FakeNewsContent/PolitiFact_Fake_'+str(i)+'-Webpage.json'\n",
        "    df = pd.read_json(path,orient='index')\n",
        "    df = df.transpose()\n",
        "    pf_fake_news = pd.concat([pf_fake_news, df])\n",
        "\n",
        "del(df,path,i,temp)\n",
        "\n",
        "\n",
        "#Assing Classe\n",
        "pf_fake_news['Real'] = 0\n",
        "pf_real_news['Real'] = 1\n",
        "\n",
        "print(pf_real_news.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>top_img</th>\n",
              "      <th>text</th>\n",
              "      <th>authors</th>\n",
              "      <th>keywords</th>\n",
              "      <th>meta_data</th>\n",
              "      <th>canonical_link</th>\n",
              "      <th>images</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>summary</th>\n",
              "      <th>movies</th>\n",
              "      <th>publish_date</th>\n",
              "      <th>source</th>\n",
              "      <th>Real</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://occupydemocrats.com/wp-content/uploads/...</td>\n",
              "      <td>335 SHARES SHARE THIS STORY\\n\\nRepublican atta...</td>\n",
              "      <td>[Colin Taylor]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'generator': 'Powered by Visual Composer - dr...</td>\n",
              "      <td>http://occupydemocrats.com/2016/01/12/virginia...</td>\n",
              "      <td>[http://occupydemocrats.com/wp-content/uploads...</td>\n",
              "      <td>Virginia Republican Wants Schools To Check Chi...</td>\n",
              "      <td>http://www.occupydemocrats.com/virginia-republ...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1452628948000}</td>\n",
              "      <td>http://www.occupydemocrats.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.americanpoliticnews.com/wp-content/...</td>\n",
              "      <td>Denzel Washington Switches to Trump Shocks Hol...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'og': {'site_name': 'American Politic', 'desc...</td>\n",
              "      <td>http://www.americanpoliticnews.com/news/denzel...</td>\n",
              "      <td>[http://www.americanpoliticnews.com/wp-content...</td>\n",
              "      <td>Denzel Washington Switches to Trump Shocks Hol...</td>\n",
              "      <td>http://www.americanpoliticnews.com/news/denzel...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1472491609000}</td>\n",
              "      <td>http://www.americanpoliticnews.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://dailysnark.com/wp-content/uploads/2016/...</td>\n",
              "      <td>Ad\\n\\nYou may asked what the Unites States did...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'generator': 'Powered by Slider Revolution 5....</td>\n",
              "      <td>http://dailysnark.com/harambe-dead-gorilla-got...</td>\n",
              "      <td>[http://dailysnark.com/wp-content/uploads/2016...</td>\n",
              "      <td>Harambe, A Dead Gorilla, Got Over 15,000 Votes...</td>\n",
              "      <td>http://dailysnark.com/harambe-dead-gorilla-got...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1478666395000}</td>\n",
              "      <td>http://dailysnark.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://cdn.vox-cdn.com/thumbor/sye0FzRVD4YBWJ...</td>\n",
              "      <td>Last night, a twitter account by the name of @...</td>\n",
              "      <td>[Nov Est]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'outbrainsection': 'us-world', 'msapplication...</td>\n",
              "      <td>https://www.theverge.com/2016/11/25/13748226/c...</td>\n",
              "      <td>[https://cdn.vox-cdn.com/thumbor/BDjczkjZSskRO...</td>\n",
              "      <td>The CNN porn scare is how fake news spreads</td>\n",
              "      <td>http://www.theverge.com/2016/11/25/13748226/cn...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1480032000000}</td>\n",
              "      <td>http://www.theverge.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://spinzon.com/wp-content/uploads/2016/11/...</td>\n",
              "      <td>Lady Gaga has opened up about the perils of fa...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'description': 'Gaga also said that she finds...</td>\n",
              "      <td>http://spinzon.com/lady-gaga-reveals-plan-cove...</td>\n",
              "      <td>[https://i0.wp.com/assets.pinterest.com/images...</td>\n",
              "      <td>Lady Gaga Reveals Plan To Cover Her Face Again...</td>\n",
              "      <td>http://spinzon.com/lady-gaga-reveals-plan-cove...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1480072046000}</td>\n",
              "      <td>http://spinzon.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             top_img  \\\n",
              "0  http://occupydemocrats.com/wp-content/uploads/...   \n",
              "0  http://www.americanpoliticnews.com/wp-content/...   \n",
              "0  http://dailysnark.com/wp-content/uploads/2016/...   \n",
              "0  https://cdn.vox-cdn.com/thumbor/sye0FzRVD4YBWJ...   \n",
              "0  http://spinzon.com/wp-content/uploads/2016/11/...   \n",
              "\n",
              "                                                text         authors keywords  \\\n",
              "0  335 SHARES SHARE THIS STORY\\n\\nRepublican atta...  [Colin Taylor]       []   \n",
              "0  Denzel Washington Switches to Trump Shocks Hol...              []       []   \n",
              "0  Ad\\n\\nYou may asked what the Unites States did...              []       []   \n",
              "0  Last night, a twitter account by the name of @...       [Nov Est]       []   \n",
              "0  Lady Gaga has opened up about the perils of fa...              []       []   \n",
              "\n",
              "                                           meta_data  \\\n",
              "0  {'generator': 'Powered by Visual Composer - dr...   \n",
              "0  {'og': {'site_name': 'American Politic', 'desc...   \n",
              "0  {'generator': 'Powered by Slider Revolution 5....   \n",
              "0  {'outbrainsection': 'us-world', 'msapplication...   \n",
              "0  {'description': 'Gaga also said that she finds...   \n",
              "\n",
              "                                      canonical_link  \\\n",
              "0  http://occupydemocrats.com/2016/01/12/virginia...   \n",
              "0  http://www.americanpoliticnews.com/news/denzel...   \n",
              "0  http://dailysnark.com/harambe-dead-gorilla-got...   \n",
              "0  https://www.theverge.com/2016/11/25/13748226/c...   \n",
              "0  http://spinzon.com/lady-gaga-reveals-plan-cove...   \n",
              "\n",
              "                                              images  \\\n",
              "0  [http://occupydemocrats.com/wp-content/uploads...   \n",
              "0  [http://www.americanpoliticnews.com/wp-content...   \n",
              "0  [http://dailysnark.com/wp-content/uploads/2016...   \n",
              "0  [https://cdn.vox-cdn.com/thumbor/BDjczkjZSskRO...   \n",
              "0  [https://i0.wp.com/assets.pinterest.com/images...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Virginia Republican Wants Schools To Check Chi...   \n",
              "0  Denzel Washington Switches to Trump Shocks Hol...   \n",
              "0  Harambe, A Dead Gorilla, Got Over 15,000 Votes...   \n",
              "0        The CNN porn scare is how fake news spreads   \n",
              "0  Lady Gaga Reveals Plan To Cover Her Face Again...   \n",
              "\n",
              "                                                 url summary movies  \\\n",
              "0  http://www.occupydemocrats.com/virginia-republ...             []   \n",
              "0  http://www.americanpoliticnews.com/news/denzel...             []   \n",
              "0  http://dailysnark.com/harambe-dead-gorilla-got...             []   \n",
              "0  http://www.theverge.com/2016/11/25/13748226/cn...             []   \n",
              "0  http://spinzon.com/lady-gaga-reveals-plan-cove...             []   \n",
              "\n",
              "               publish_date                              source  Real  \n",
              "0  {'$date': 1452628948000}      http://www.occupydemocrats.com     0  \n",
              "0  {'$date': 1472491609000}  http://www.americanpoliticnews.com     0  \n",
              "0  {'$date': 1478666395000}               http://dailysnark.com     0  \n",
              "0  {'$date': 1480032000000}             http://www.theverge.com     0  \n",
              "0  {'$date': 1480072046000}                  http://spinzon.com     0  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pf_real_news = pf_real_news.reset_index(drop=True)\n",
        "pf_fake_news.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dc2Y1J5E4dW"
      },
      "source": [
        "There are two news content sets for stories categorized by Politifact one consisting of fake and real stories. Both data sets contain 120 stories with the same 13 fields for each. Above is an informational readout for the Politifact fake news data with the name of each field where the names are mostly self-expanatory. Note that the text column contains the entire text of the news article.\n",
        "\n",
        "### Politifact: Load and Format User/Network Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ClEeyiFE4dZ",
        "outputId": "c4738612-eb82-4026-e809-3676d2f92360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of nodes in the network is  23866\n",
            "The number of edges in the network is  574744\n",
            "The number of user-news relationships is 32791\n"
          ]
        }
      ],
      "source": [
        "# get user info data\n",
        "pf_news = pd.read_csv('PolitiFact/News.txt',header=None,names=['news'])\n",
        "pf_users = pd.read_csv('PolitiFact/User.txt',header=None,names=['users'])\n",
        "pf_news_user = pd.read_csv('PolitiFact/PolitiFactNewsUser.txt',header=None,names=['news_users'])\n",
        "pf_user_user = pd.read_csv('PolitiFact/PolitiFactUserUser.txt',header=None,names=['followers'])\n",
        "\n",
        "# build graph\n",
        "G_pf = nx.DiGraph()\n",
        "G_pf.add_nodes_from(pf_users)\n",
        "G_pf.add_edges_from(pf_user_user['followers'].str.split('\\t'))\n",
        "print('The number of nodes in the network is ',G_pf.number_of_nodes())\n",
        "print('The number of edges in the network is ',G_pf.number_of_edges())\n",
        "print('The number of user-news relationships is', len(pf_news_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdt2HHJtUKaS"
      },
      "source": [
        "There are 23,866 distinct twitter users represented in the data set. \n",
        "\n",
        "There are 574,744 user relationships that represent who is following who. \n",
        "\n",
        "Finally there are 32,791 user-news relationships that tell which user shared which story and how many times the story was shared.\n",
        "\n",
        "### BuzzFeed: Load and Format News Content Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98lTOETjUOZq",
        "outputId": "4e754c33-8aa0-4dbd-9f41-ea7815c856fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 91 entries, 0 to 0\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   top_img         91 non-null     object\n",
            " 1   text            91 non-null     object\n",
            " 2   authors         91 non-null     object\n",
            " 3   keywords        91 non-null     object\n",
            " 4   meta_data       91 non-null     object\n",
            " 5   canonical_link  91 non-null     object\n",
            " 6   images          91 non-null     object\n",
            " 7   title           91 non-null     object\n",
            " 8   url             91 non-null     object\n",
            " 9   summary         91 non-null     object\n",
            " 10  movies          91 non-null     object\n",
            " 11  publish_date    56 non-null     object\n",
            " 12  source          91 non-null     object\n",
            " 13  Real            91 non-null     int64 \n",
            "dtypes: int64(1), object(13)\n",
            "memory usage: 10.7+ KB\n"
          ]
        }
      ],
      "source": [
        "#get real news content data\n",
        "temp = pd.read_json('BuzzFeed/RealNewsContent/BuzzFeed_Real_2-Webpage.json',orient='index')\n",
        "bf_real_news =  pd.DataFrame(columns=temp.index)\n",
        "for i in range(1,92):\n",
        "    path='BuzzFeed/RealNewsContent/BuzzFeed_Real_'+str(i)+'-Webpage.json'\n",
        "    df = pd.read_json(path,orient='index')\n",
        "    df = df.transpose()\n",
        "    bf_real_news = pd.concat([bf_real_news,df])\n",
        "del(df,path,i)\n",
        "\n",
        "#get fake news content data\n",
        "bf_fake_news =  pd.DataFrame(columns=temp.index)\n",
        "for i in range(1,92):\n",
        "    path='BuzzFeed/FakeNewsContent/BuzzFeed_Fake_'+str(i)+'-Webpage.json'\n",
        "    df = pd.read_json(path,orient='index')\n",
        "    df = df.transpose()\n",
        "    bf_fake_news = pd.concat([bf_fake_news, df])\n",
        "del(df,path,i,temp)\n",
        "bf_fake_news['Real'] = 0\n",
        "bf_real_news['Real'] = 1\n",
        "\n",
        "bf_real_news.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "8Lg3fC4R4XIK",
        "outputId": "cc81594a-c0ec-43e0-b122-8c43603fb6dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>top_img</th>\n",
              "      <th>text</th>\n",
              "      <th>authors</th>\n",
              "      <th>keywords</th>\n",
              "      <th>meta_data</th>\n",
              "      <th>canonical_link</th>\n",
              "      <th>images</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>summary</th>\n",
              "      <th>movies</th>\n",
              "      <th>publish_date</th>\n",
              "      <th>source</th>\n",
              "      <th>Real</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://addictinginfo.addictinginfoent.netdna-c...</td>\n",
              "      <td>I woke up this morning to find a variation of ...</td>\n",
              "      <td>[Wendy Gittleson]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'publisher': 'Addicting Info | The Knowledge ...</td>\n",
              "      <td>http://addictinginfo.com/2016/09/19/proof-the-...</td>\n",
              "      <td>[http://i.imgur.com/JeqZLhj.png, http://addict...</td>\n",
              "      <td>Proof The Mainstream Media Is Manipulating The...</td>\n",
              "      <td>http://www.addictinginfo.org/2016/09/19/proof-...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1474243200000}</td>\n",
              "      <td>http://www.addictinginfo.org</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://usherald.com/wp-content/uploads/2015/05...</td>\n",
              "      <td>Thanks in part to the declassification of Defe...</td>\n",
              "      <td>[Bob Amoroso]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'generator': 'WordPress 4.8.1', 'og': {'site_...</td>\n",
              "      <td>http://usherald.com/breaking-declassified-docs...</td>\n",
              "      <td>[http://usherald.com/wp-content/uploads/2015/0...</td>\n",
              "      <td>Declassified Docs Show That Obama Admin Create...</td>\n",
              "      <td>http://usherald.com/breaking-declassified-docs...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1432650030000}</td>\n",
              "      <td>http://usherald.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://eaglerising.com/wp-content/uploads/2016...</td>\n",
              "      <td>The Democrats are using an intimidation tactic...</td>\n",
              "      <td>[View All Posts]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'description': 'There is evidence the birth c...</td>\n",
              "      <td>http://eaglerising.com/36841/why-is-it-racist-...</td>\n",
              "      <td>[http://2lv0hm3wvpix464wwy2zh7d1.wpengine.netd...</td>\n",
              "      <td>Why is it “RACIST” to Question Someone’s Birth...</td>\n",
              "      <td>http://eaglerising.com/36841/why-is-it-racist-...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1474243356000}</td>\n",
              "      <td>http://eaglerising.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>Dolly Kyle has written a scathing “tell all” b...</td>\n",
              "      <td>[Fed Up]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'generator': 'Po...</td>\n",
              "      <td>http://100percentfedup.com/hillary-on-disabled...</td>\n",
              "      <td>[https://www.facebook.com/tr?id=15790889156864...</td>\n",
              "      <td>HILLARY ON DISABLED CHILDREN During Easter Egg...</td>\n",
              "      <td>http://100percentfedup.com/hillary-on-disabled...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1466439263000}</td>\n",
              "      <td>http://100percentfedup.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://clashdaily.com/wp-content/uploads/2016/...</td>\n",
              "      <td>The Haitians in the audience have some newswor...</td>\n",
              "      <td>[Rich Witmer, Doug Giles]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'og': {'site_nam...</td>\n",
              "      <td>http://clashdaily.com/2016/09/watch-trump-visi...</td>\n",
              "      <td>[http://clashdaily.wpengine.netdna-cdn.com/wp-...</td>\n",
              "      <td>'Reporters' FLEE When Clintons Get EXPOSED!</td>\n",
              "      <td>http://clashdaily.com/2016/09/watch-trump-visi...</td>\n",
              "      <td></td>\n",
              "      <td>[https://www.youtube.com/embed/x5IS6Ya005E?fea...</td>\n",
              "      <td>{'$date': 1474208802000}</td>\n",
              "      <td>http://clashdaily.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>http://rightwingnews.com/wp-content/uploads/20...</td>\n",
              "      <td>BREAKING: Steps to FORCE FBI Director Comey to...</td>\n",
              "      <td>[Cassy Fiano]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'og': {'site_nam...</td>\n",
              "      <td>http://rightwingnews.com/hillary-clinton-2/bre...</td>\n",
              "      <td>[http://rightwingnews.com/wp-content/uploads/2...</td>\n",
              "      <td>BREAKING: Steps to FORCE FBI Director Comey to...</td>\n",
              "      <td>http://rightwingnews.com/hillary-clinton-2/bre...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1474907754000}</td>\n",
              "      <td>http://rightwingnews.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>http://rightwingnews.com/wp-content/uploads/20...</td>\n",
              "      <td>Hillary’s TOP Donor Country Just Auctioned Off...</td>\n",
              "      <td>[Terresa Monroe-hamilton]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'og': {'site_nam...</td>\n",
              "      <td>http://rightwingnews.com/hillary-clinton-2/hil...</td>\n",
              "      <td>[http://1.gravatar.com/avatar/d35b77ff6c390071...</td>\n",
              "      <td>Hillary’s TOP Donor Country Just Auctioned Off...</td>\n",
              "      <td>http://rightwingnews.com/hillary-clinton-2/hil...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1474912025000}</td>\n",
              "      <td>http://rightwingnews.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>http://conservativetribune.com/wp-content/uplo...</td>\n",
              "      <td>Advertisement - story continues below\\n\\nThe f...</td>\n",
              "      <td>[Martin Lioll, John Falkenberg, Ben Marquis, K...</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'description': 'People are already calling th...</td>\n",
              "      <td>http://conservativetribune.com/lester-holt-lie...</td>\n",
              "      <td>[http://conservativetribune.com/wp-content/upl...</td>\n",
              "      <td>Cavuto Just Exposed Lester Holt's Lies During ...</td>\n",
              "      <td>http://conservativetribune.com/lester-holt-lie...</td>\n",
              "      <td></td>\n",
              "      <td>[https://www.youtube.com/embed/ThwaDSaoGU8?fea...</td>\n",
              "      <td>{'$date': 1474934400000}</td>\n",
              "      <td>http://conservativetribune.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>http://assets.thepoliticalinsider.com.s3.amazo...</td>\n",
              "      <td>\\n\\nThere’s a lot to be discussed about last n...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'description': 'One thing that has baffled ma...</td>\n",
              "      <td>http://thepoliticalinsider.com/first-president...</td>\n",
              "      <td>[http://1.gravatar.com/avatar/71be986d321b3d52...</td>\n",
              "      <td>People Noticed Something Odd About Hillary's O...</td>\n",
              "      <td>http://www.thepoliticalinsider.com/first-presi...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1475000011000}</td>\n",
              "      <td>http://www.thepoliticalinsider.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>http://rightwingnews.com/wp-content/uploads/20...</td>\n",
              "      <td>People Noticed Something Odd About Hillary’s O...</td>\n",
              "      <td>[Lisa Smith]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'og': {'site_nam...</td>\n",
              "      <td>http://rightwingnews.com/top-news/people-notic...</td>\n",
              "      <td>[http://0.gravatar.com/avatar/3728144c20aefded...</td>\n",
              "      <td>People Noticed Something Odd About Hillary’s O...</td>\n",
              "      <td>http://rightwingnews.com/top-news/people-notic...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>{'$date': 1475010040000}</td>\n",
              "      <td>http://rightwingnews.com</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              top_img  \\\n",
              "0   http://addictinginfo.addictinginfoent.netdna-c...   \n",
              "1   http://usherald.com/wp-content/uploads/2015/05...   \n",
              "2   http://eaglerising.com/wp-content/uploads/2016...   \n",
              "3   http://100percentfedup.com/wp-content/uploads/...   \n",
              "4   http://clashdaily.com/wp-content/uploads/2016/...   \n",
              "..                                                ...   \n",
              "86  http://rightwingnews.com/wp-content/uploads/20...   \n",
              "87  http://rightwingnews.com/wp-content/uploads/20...   \n",
              "88  http://conservativetribune.com/wp-content/uplo...   \n",
              "89  http://assets.thepoliticalinsider.com.s3.amazo...   \n",
              "90  http://rightwingnews.com/wp-content/uploads/20...   \n",
              "\n",
              "                                                 text  \\\n",
              "0   I woke up this morning to find a variation of ...   \n",
              "1   Thanks in part to the declassification of Defe...   \n",
              "2   The Democrats are using an intimidation tactic...   \n",
              "3   Dolly Kyle has written a scathing “tell all” b...   \n",
              "4   The Haitians in the audience have some newswor...   \n",
              "..                                                ...   \n",
              "86  BREAKING: Steps to FORCE FBI Director Comey to...   \n",
              "87  Hillary’s TOP Donor Country Just Auctioned Off...   \n",
              "88  Advertisement - story continues below\\n\\nThe f...   \n",
              "89  \\n\\nThere’s a lot to be discussed about last n...   \n",
              "90  People Noticed Something Odd About Hillary’s O...   \n",
              "\n",
              "                                              authors keywords  \\\n",
              "0                                   [Wendy Gittleson]       []   \n",
              "1                                       [Bob Amoroso]       []   \n",
              "2                                    [View All Posts]       []   \n",
              "3                                            [Fed Up]       []   \n",
              "4                           [Rich Witmer, Doug Giles]       []   \n",
              "..                                                ...      ...   \n",
              "86                                      [Cassy Fiano]       []   \n",
              "87                          [Terresa Monroe-hamilton]       []   \n",
              "88  [Martin Lioll, John Falkenberg, Ben Marquis, K...       []   \n",
              "89                                                 []       []   \n",
              "90                                       [Lisa Smith]       []   \n",
              "\n",
              "                                            meta_data  \\\n",
              "0   {'publisher': 'Addicting Info | The Knowledge ...   \n",
              "1   {'generator': 'WordPress 4.8.1', 'og': {'site_...   \n",
              "2   {'description': 'There is evidence the birth c...   \n",
              "3   {'googlebot': 'noimageindex', 'generator': 'Po...   \n",
              "4   {'googlebot': 'noimageindex', 'og': {'site_nam...   \n",
              "..                                                ...   \n",
              "86  {'googlebot': 'noimageindex', 'og': {'site_nam...   \n",
              "87  {'googlebot': 'noimageindex', 'og': {'site_nam...   \n",
              "88  {'description': 'People are already calling th...   \n",
              "89  {'description': 'One thing that has baffled ma...   \n",
              "90  {'googlebot': 'noimageindex', 'og': {'site_nam...   \n",
              "\n",
              "                                       canonical_link  \\\n",
              "0   http://addictinginfo.com/2016/09/19/proof-the-...   \n",
              "1   http://usherald.com/breaking-declassified-docs...   \n",
              "2   http://eaglerising.com/36841/why-is-it-racist-...   \n",
              "3   http://100percentfedup.com/hillary-on-disabled...   \n",
              "4   http://clashdaily.com/2016/09/watch-trump-visi...   \n",
              "..                                                ...   \n",
              "86  http://rightwingnews.com/hillary-clinton-2/bre...   \n",
              "87  http://rightwingnews.com/hillary-clinton-2/hil...   \n",
              "88  http://conservativetribune.com/lester-holt-lie...   \n",
              "89  http://thepoliticalinsider.com/first-president...   \n",
              "90  http://rightwingnews.com/top-news/people-notic...   \n",
              "\n",
              "                                               images  \\\n",
              "0   [http://i.imgur.com/JeqZLhj.png, http://addict...   \n",
              "1   [http://usherald.com/wp-content/uploads/2015/0...   \n",
              "2   [http://2lv0hm3wvpix464wwy2zh7d1.wpengine.netd...   \n",
              "3   [https://www.facebook.com/tr?id=15790889156864...   \n",
              "4   [http://clashdaily.wpengine.netdna-cdn.com/wp-...   \n",
              "..                                                ...   \n",
              "86  [http://rightwingnews.com/wp-content/uploads/2...   \n",
              "87  [http://1.gravatar.com/avatar/d35b77ff6c390071...   \n",
              "88  [http://conservativetribune.com/wp-content/upl...   \n",
              "89  [http://1.gravatar.com/avatar/71be986d321b3d52...   \n",
              "90  [http://0.gravatar.com/avatar/3728144c20aefded...   \n",
              "\n",
              "                                                title  \\\n",
              "0   Proof The Mainstream Media Is Manipulating The...   \n",
              "1   Declassified Docs Show That Obama Admin Create...   \n",
              "2   Why is it “RACIST” to Question Someone’s Birth...   \n",
              "3   HILLARY ON DISABLED CHILDREN During Easter Egg...   \n",
              "4         'Reporters' FLEE When Clintons Get EXPOSED!   \n",
              "..                                                ...   \n",
              "86  BREAKING: Steps to FORCE FBI Director Comey to...   \n",
              "87  Hillary’s TOP Donor Country Just Auctioned Off...   \n",
              "88  Cavuto Just Exposed Lester Holt's Lies During ...   \n",
              "89  People Noticed Something Odd About Hillary's O...   \n",
              "90  People Noticed Something Odd About Hillary’s O...   \n",
              "\n",
              "                                                  url summary  \\\n",
              "0   http://www.addictinginfo.org/2016/09/19/proof-...           \n",
              "1   http://usherald.com/breaking-declassified-docs...           \n",
              "2   http://eaglerising.com/36841/why-is-it-racist-...           \n",
              "3   http://100percentfedup.com/hillary-on-disabled...           \n",
              "4   http://clashdaily.com/2016/09/watch-trump-visi...           \n",
              "..                                                ...     ...   \n",
              "86  http://rightwingnews.com/hillary-clinton-2/bre...           \n",
              "87  http://rightwingnews.com/hillary-clinton-2/hil...           \n",
              "88  http://conservativetribune.com/lester-holt-lie...           \n",
              "89  http://www.thepoliticalinsider.com/first-presi...           \n",
              "90  http://rightwingnews.com/top-news/people-notic...           \n",
              "\n",
              "                                               movies  \\\n",
              "0                                                  []   \n",
              "1                                                  []   \n",
              "2                                                  []   \n",
              "3                                                  []   \n",
              "4   [https://www.youtube.com/embed/x5IS6Ya005E?fea...   \n",
              "..                                                ...   \n",
              "86                                                 []   \n",
              "87                                                 []   \n",
              "88  [https://www.youtube.com/embed/ThwaDSaoGU8?fea...   \n",
              "89                                                 []   \n",
              "90                                                 []   \n",
              "\n",
              "                publish_date                              source  Real  \n",
              "0   {'$date': 1474243200000}        http://www.addictinginfo.org     0  \n",
              "1   {'$date': 1432650030000}                 http://usherald.com     0  \n",
              "2   {'$date': 1474243356000}              http://eaglerising.com     0  \n",
              "3   {'$date': 1466439263000}          http://100percentfedup.com     0  \n",
              "4   {'$date': 1474208802000}               http://clashdaily.com     0  \n",
              "..                       ...                                 ...   ...  \n",
              "86  {'$date': 1474907754000}            http://rightwingnews.com     0  \n",
              "87  {'$date': 1474912025000}            http://rightwingnews.com     0  \n",
              "88  {'$date': 1474934400000}      http://conservativetribune.com     0  \n",
              "89  {'$date': 1475000011000}  http://www.thepoliticalinsider.com     0  \n",
              "90  {'$date': 1475010040000}            http://rightwingnews.com     0  \n",
              "\n",
              "[91 rows x 14 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bf_fake_news = bf_fake_news.reset_index(drop=True)\n",
        "bf_fake_news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDVwkdo3lN6H"
      },
      "source": [
        "The BuzzFeed set is organized in the same fashion as the Politifact dataset with the same features. In this set though, we have only 91 stories for each set for a total of 182 stories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpsbi-OtUYXM"
      },
      "source": [
        "### BuzzFeed: Load and Format User/Network Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ekBewJQUgfZ",
        "outputId": "24c7d4a4-dd78-4a10-e780-0dfa9018c6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of nodes in the network is  15258\n",
            "The number of edges in the network is  634750\n",
            "The number of user-news relationships is 22779\n"
          ]
        }
      ],
      "source": [
        "# get user info data\n",
        "bf_news = pd.read_csv('BuzzFeed/News.txt',header=None,names=['news'])\n",
        "bf_users = pd.read_csv('BuzzFeed/User.txt',header=None,names=['users'])\n",
        "bf_news_user = pd.read_csv('BuzzFeed/BuzzFeedNewsUser.txt',header=None,names=['news_users'])\n",
        "bf_user_user = pd.read_csv('BuzzFeed/BuzzFeedUserUser.txt',header=None,names=['followers'])\n",
        "\n",
        "# build graph\n",
        "G_bf = nx.DiGraph()\n",
        "G_bf.add_nodes_from(bf_users)\n",
        "G_bf.add_edges_from(bf_user_user['followers'].str.split('\\t'))\n",
        "print('The number of nodes in the network is ',G_bf.number_of_nodes())\n",
        "print('The number of edges in the network is ',G_bf.number_of_edges())\n",
        "print('The number of user-news relationships is', len(bf_news_user))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4HkGb6zkGMu"
      },
      "source": [
        "In this network we have 15,258 distinct twitter users and 634,750 user relationships represented in the data set. Finally there are 22,779 user-news relationships that tell which user shared which story and how many times.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX3jYg6cC60o"
      },
      "source": [
        "## Feature Engineering: Social Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN0XtwEEC71X"
      },
      "source": [
        "### Number of Times Shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L1l8vhbFC7h0"
      },
      "outputs": [],
      "source": [
        "# Politifact\n",
        "pf_news = pd. concat([pf_real_news, pf_fake_news]).reset_index(drop=True)\n",
        "pf_news.index\n",
        "pf_news['news_number']  = pf_news.index +1\n",
        "pf_news_user['news_users']=pf_news_user['news_users'].str.split('\\t')\n",
        "pf_news_user=pf_news_user['news_users'].apply(pd.Series)\n",
        "pf_news_user.rename(columns={0:'news',1:'user',2:'num_shared'},inplace=True)\n",
        "pf_news_user['num_shared'] = pf_news_user['num_shared'].astype('int')\n",
        "pf_news_user['news'] = pf_news_user['news'].astype('int')\n",
        "df=pf_news_user.groupby('news')['num_shared'].sum()\n",
        "df=df.reset_index(drop=True)\n",
        "pf_news['num_shared']=df\n",
        "del(df)\n",
        "pf_news['num_shared'] = pf_news['num_shared'].astype('int')\n",
        "\n",
        "# BuzzFeed\n",
        "bf_news = pd.concat([bf_real_news,bf_fake_news]).reset_index(drop=True)\n",
        "bf_news.index\n",
        "bf_news['news_number']  = bf_news.index +1\n",
        "bf_news['news_number']\n",
        "bf_news_user['news_users']=bf_news_user['news_users'].str.split('\\t')\n",
        "bf_news_user=bf_news_user['news_users'].apply(pd.Series)\n",
        "bf_news_user.rename(columns={0:'news',1:'user',2:'num_shared'},inplace=True)\n",
        "bf_news_user['num_shared'] = bf_news_user['num_shared'].astype('int')\n",
        "bf_news_user['news'] = bf_news_user['news'].astype('int')\n",
        "df=bf_news_user.groupby('news')['num_shared'].sum()\n",
        "df=df.reset_index(drop=True)\n",
        "bf_news['num_shared']=df\n",
        "del(df)\n",
        "bf_news['num_shared'] = bf_news['num_shared'].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i-E-lvtAz23"
      },
      "source": [
        "### Num Times Shared by top 2 percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehqy1f85A0N7",
        "outputId": "7c55edbf-2e01-4163-c86b-221079923987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-477\n",
            "-305\n"
          ]
        }
      ],
      "source": [
        "# Politifact\n",
        "degree_cent = nx.in_degree_centrality(G_pf)\n",
        "sort_dict_degree= dict(sorted((value, key) for (key,value) in degree_cent.items())) \n",
        "top= -int(.02*G_pf.number_of_nodes())\n",
        "print(top)\n",
        "temp=list(sort_dict_degree.values())[top:]\n",
        "foo=pf_news_user.loc[pf_news_user['user'].isin(temp),'news'].sort_values()\n",
        "foo1=foo.groupby(foo).count()\n",
        "foo1.index=foo1.index-1\n",
        "pf_news['shared_by_top']=foo1\n",
        "pf_news.loc[pf_news['shared_by_top'].isna(),'shared_by_top']=0\n",
        "# pf_news[['shared_by_top','Real']]\n",
        "\n",
        "# Buzzfeed\n",
        "degree_cent = nx.in_degree_centrality(G_bf)\n",
        "sort_dict_degree= dict(sorted((value, key) for (key,value) in degree_cent.items())) \n",
        "top= -int(.02*G_bf.number_of_nodes())\n",
        "print(top)\n",
        "temp=list(sort_dict_degree.values())[top:]\n",
        "foo=bf_news_user.loc[bf_news_user['user'].isin(temp),'news'].sort_values()\n",
        "foo1=foo.groupby(foo).count()\n",
        "foo1.index=foo1.index-1\n",
        "bf_news['shared_by_top']=foo1\n",
        "bf_news.loc[bf_news['shared_by_top'].isna(),'shared_by_top']=0\n",
        "#bf_news['shared_by_top']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UrHHgSSA-RS"
      },
      "source": [
        "### Avg. Number of Followers Shared by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6JUYQepSA-bu"
      },
      "outputs": [],
      "source": [
        "# Politifact\n",
        "degree=pd.DataFrame.from_dict(G_pf.in_degree).drop(0).rename(columns={0:'user',1:'followers'})\n",
        "degree.reset_index(drop=True,inplace=True)\n",
        "degree['user']=degree['user'].astype('int')\n",
        "pf_news_user['user']=pf_news_user['user'].astype('int')\n",
        "df=degree.merge(pf_news_user,on='user')\n",
        "df=df.groupby('news')['followers'].mean().reset_index()\n",
        "pf_news['avg_follower']=df['followers']\n",
        "del(df)\n",
        "pf_news['avg_follower']\n",
        "\n",
        "# Buzzfeed\n",
        "degree=pd.DataFrame.from_dict(G_bf.in_degree).drop(0).rename(columns={0:'user',1:'followers'})\n",
        "degree.reset_index(drop=True,inplace=True)\n",
        "degree['user']=degree['user'].astype('int')\n",
        "bf_news_user['user']=bf_news_user['user'].astype('int')\n",
        "df=degree.merge(bf_news_user,on='user')\n",
        "df=df.groupby('news')['followers'].mean().reset_index()\n",
        "bf_news['avg_follower']=df['followers']\n",
        "del(df)\n",
        "#bf_news['avg_follower']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbAtAN4fBOzq"
      },
      "source": [
        "### Avg. Number of followees shared by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "R8sMBdB_BWgE"
      },
      "outputs": [],
      "source": [
        "# Politifact\n",
        "degree=pd.DataFrame.from_dict(G_pf.out_degree).drop(0).rename(columns={0:'user',1:'followees'})\n",
        "degree.reset_index(drop=True,inplace=True)\n",
        "degree['user']=degree['user'].astype('int')\n",
        "#pf_news_user['user']=pf_news_user['user'].astype('int')\n",
        "df=degree.merge(pf_news_user,on='user')\n",
        "df=df.groupby('news')['followees'].mean().reset_index()\n",
        "pf_news['avg_followee']=df['followees']\n",
        "del(df)\n",
        "#pf_news['avg_followee']\n",
        "\n",
        "# BuzzFeed\n",
        "degree=pd.DataFrame.from_dict(G_bf.out_degree).drop(0).rename(columns={0:'user',1:'followees'})\n",
        "degree.reset_index(drop=True,inplace=True)\n",
        "degree['user']=degree['user'].astype('int')\n",
        "#bf_news_user['user']=bf_news_user['user'].astype('int')\n",
        "df=degree.merge(bf_news_user,on='user')\n",
        "df=df.groupby('news')['followees'].mean().reset_index()\n",
        "bf_news['avg_followee']=df['followees']\n",
        "del(df)\n",
        "#bf_news['avg_followee']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mDEBfr_BZqm"
      },
      "source": [
        "### Followee to Follower Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HEPKq6PvBZ2n"
      },
      "outputs": [],
      "source": [
        "pf_news['f_ratio']=pf_news['avg_followee']/pf_news['avg_follower']\n",
        "bf_news['f_ratio']=bf_news['avg_followee']/bf_news['avg_follower']\n",
        "#bf_news['f_ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.head()# news = news.append([bf_news,pf_news],ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15257 entries, 1 to 15257\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   user       15257 non-null  int64  \n",
            " 1   Followers  15257 non-null  int64  \n",
            " 2   Fraction   15257 non-null  float64\n",
            "dtypes: float64(1), int64(2)\n",
            "memory usage: 357.7 KB\n"
          ]
        }
      ],
      "source": [
        "# df['user'] = df.user.astype(int)\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22779 entries, 0 to 22778\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype\n",
            "---  ------      --------------  -----\n",
            " 0   news        22779 non-null  int64\n",
            " 1   user        22779 non-null  int64\n",
            " 2   num_shared  22779 non-null  int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 534.0 KB\n"
          ]
        }
      ],
      "source": [
        "bf_news_user.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QUyuMK64Yom8"
      },
      "outputs": [],
      "source": [
        "# G_bf.degree\n",
        "# G_bf_outD = dict(G_bf.out_degree())\n",
        "# k = [k for k,v in G_bf_outD.items()]\n",
        "# v = [v for k,v in G_bf_outD.items()]\n",
        "# df = pd.DataFrame({'user': k, 'Followers': v, 'Fraction': v/np.sum(v)}).drop(0,axis=0)\n",
        "# df_ = df.merge(bf_news_user, on='user')\n",
        "# bf_news_user.head()\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "36504fD1nMZw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Users</th>\n",
              "      <th>Followers</th>\n",
              "      <th>Fraction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>899</td>\n",
              "      <td>19</td>\n",
              "      <td>0.000030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6781</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10097</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Users  Followers  Fraction\n",
              "1     48          2  0.000003\n",
              "2      1          4  0.000006\n",
              "3    899         19  0.000030\n",
              "4   6781          3  0.000005\n",
              "5  10097         10  0.000016"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G_bf.degree\n",
        "G_bf_inD = dict(G_bf.in_degree())\n",
        "k = [k for k,v in G_bf_inD.items()]\n",
        "v = [v for k,v in G_bf_inD.items()]\n",
        "df = pd.DataFrame({'Users': k, 'Followers': v, 'Fraction': v/np.sum(v)}).drop(0,axis=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cOylypfHdXc"
      },
      "source": [
        "**Betweeness Centrality for Buzz Feed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pw1Y6NQiE2E3"
      },
      "outputs": [],
      "source": [
        "# G_bf_b = nx.betweenness_centrality(G_bf, endpoints=True,normalized=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_G_bf_b = G_bf_b.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_G_bf_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4S_XCcHqGz"
      },
      "source": [
        "**Closeness Centrality for Buzz Feed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDBcRsQOtQS8"
      },
      "outputs": [],
      "source": [
        "# G_bf_c = nx.closeness_centrality(G_bf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsVH4kbHxtB"
      },
      "source": [
        "**Betweenness Centrality for Politifact**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLIfVi95tRvr"
      },
      "outputs": [],
      "source": [
        "# G_pf_b = nx.betweenness_centrality(G_pf, endpoints=True,normalized=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XjwHeq0H7z_"
      },
      "source": [
        "**Closeness Centrality for Politifact**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VARWQyZEs64D"
      },
      "outputs": [],
      "source": [
        "# G_pf_c = nx.closeness_centrality(G_pf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PpAaXlx3QeQY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with open('G_bf_b.csv', 'w') as csv_file:  \n",
        "#     writer = csv.writer(csv_file)\n",
        "#     for key, value in G_bf_b.items():\n",
        "      #  writer.writerow([key, value])\n",
        "with open('Term Project/G_bf_b.csv') as csv_file:\n",
        "    file = csv.reader(csv_file)\n",
        "    G_bf_b = dict(file)\n",
        "# G_bf_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRkRL4BISo3j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rztzJ-kZSmbl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "icyfEtjMQkp4"
      },
      "outputs": [],
      "source": [
        "# with open('G_bf_c.csv', 'w') as csv_file:  \n",
        "#     writer = csv.writer(csv_file)\n",
        "#     for key, value in G_bf_c.items():\n",
        "#        writer.writerow([key, value])\n",
        "with open('Term Project/G_bf_c.csv') as csv_file:\n",
        "    file = csv.reader(csv_file)\n",
        "    G_bf_c  = dict(file)\n",
        "# G_bf_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-Ktky6BEROod"
      },
      "outputs": [],
      "source": [
        "# with open('G_pf_b.csv', 'w') as csv_file:  \n",
        "#     writer = csv.writer(csv_file)\n",
        "#     for key, value in G_pf_b .items():\n",
        "#        writer.writerow([key, value])\n",
        "with open('Term Project/G_pf_b.csv') as csv_file:\n",
        "    file = csv.reader(csv_file)\n",
        "    G_pf_b = dict(file)\n",
        "# G_pf_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PAZqTKPDROzH"
      },
      "outputs": [],
      "source": [
        "# with open('G_pf_c.csv', 'w') as csv_file:  \n",
        "#     writer = csv.writer(csv_file)\n",
        "#     for key, value in G_pf_c.items():\n",
        "#        writer.writerow([key, value])\n",
        "with open('Term Project/G_pf_c.csv') as csv_file:\n",
        "    file = csv.reader(csv_file)\n",
        "    G_pf_c = dict(file)\n",
        "# G_pf_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MWyFIgk-E4Is"
      },
      "outputs": [],
      "source": [
        "#Buzz Feed betweenness centrality\n",
        "btw_central = G_bf_b\n",
        "\n",
        "k = [k for k,v in btw_central.items()]\n",
        "v = [v for k,v in btw_central.items()]\n",
        "btw_central = pd.DataFrame({'user': k, 'betweenness_centrality': v}).rename(columns = {0:'user',1:'betweenness_centrality'}).drop(0,axis = 0).reset_index()\n",
        "btw_central['user'] = btw_central['user'].astype('i4')\n",
        "btw_central['betweenness_centrality'] = btw_central['betweenness_centrality'].astype('float')\n",
        "df = btw_central.merge(bf_news_user, on = 'user')\n",
        "df = df.groupby('news')['betweenness_centrality'].mean().reset_index()\n",
        "bf_news['betweenness_centrality'] = df['betweenness_centrality']\n",
        "# bf_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LdLb2htVHLK-"
      },
      "outputs": [],
      "source": [
        "#Buzz Feed closenness centrality\n",
        "close_central = G_bf_c\n",
        "\n",
        "k = [k for k,v in close_central.items()]\n",
        "v = [v for k,v in close_central.items()]\n",
        "\n",
        "close_central  = pd.DataFrame({'user': k, 'closenness_centrality': v}).rename(columns = {0:'user',1:'closenness_centrality'}).drop(0,axis = 0).reset_index()\n",
        "close_central['user'] = close_central['user'].astype('i4')\n",
        "close_central['closenness_centrality'] = close_central['closenness_centrality'].astype('float')\n",
        "df = close_central.merge(bf_news_user, on = 'user')\n",
        "df = df.groupby('news')['closenness_centrality'].mean().reset_index()\n",
        "bf_news['closenness_centrality'] = df['closenness_centrality']\n",
        "# bf_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nWuRuJ6Df_PU"
      },
      "outputs": [],
      "source": [
        "#Plotifact betweenness centrality\n",
        "btw_central = G_pf_b\n",
        "\n",
        "k = [k for k,v in btw_central.items()]\n",
        "v = [v for k,v in btw_central.items()]\n",
        "\n",
        "btw_central = pd.DataFrame({'user': k, 'betweenness_centrality': v}).rename(columns = {0:'user',1:'betweenness_centrality'}).drop(0,axis = 0).reset_index()\n",
        "btw_central['user'] = btw_central['user'].astype('i4')\n",
        "btw_central['betweenness_centrality'] = btw_central['betweenness_centrality'].astype('float')\n",
        "df = btw_central.merge(pf_news_user, on = 'user')\n",
        "df = df.groupby('news')['betweenness_centrality'].mean().reset_index()\n",
        "pf_news['betweenness_centrality'] = df['betweenness_centrality']\n",
        "# pf_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "F6C2sqC9LCWg"
      },
      "outputs": [],
      "source": [
        "#Politifact closeness centrality\n",
        "close_central = G_pf_c\n",
        "\n",
        "k = [k for k,v in close_central.items()]\n",
        "v = [v for k,v in close_central.items()]\n",
        "\n",
        "close_central = pd.DataFrame({'user': k, 'closenness_centrality': v}).rename(columns = {0:'user',1:'closenness_centrality'}).drop(0,axis = 0).reset_index()\n",
        "close_central['user'] = close_central['user'].astype('i4')\n",
        "close_central['closenness_centrality'] = close_central['closenness_centrality'].astype('float')\n",
        "df = close_central.merge(pf_news_user, on = 'user')\n",
        "df = df.groupby('news')['closenness_centrality'].mean().reset_index()\n",
        "pf_news['closenness_centrality'] = df['closenness_centrality']\n",
        "# pf_news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvKxEVEQHP_z"
      },
      "source": [
        "## Union of News Content Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ-KbQUNG3Wr",
        "outputId": "ce575985-898c-4465-8266-edefd14c7891"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>top_img</th>\n",
              "      <th>text</th>\n",
              "      <th>authors</th>\n",
              "      <th>keywords</th>\n",
              "      <th>meta_data</th>\n",
              "      <th>canonical_link</th>\n",
              "      <th>images</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>summary</th>\n",
              "      <th>...</th>\n",
              "      <th>source</th>\n",
              "      <th>Real</th>\n",
              "      <th>news_number</th>\n",
              "      <th>num_shared</th>\n",
              "      <th>shared_by_top</th>\n",
              "      <th>avg_follower</th>\n",
              "      <th>avg_followee</th>\n",
              "      <th>f_ratio</th>\n",
              "      <th>betweenness_centrality</th>\n",
              "      <th>closenness_centrality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://conservativetribune.com/wp-content/uplo...</td>\n",
              "      <td>Olympic Committee Buckles to LGBT Groups... Ma...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'viewport': 'width=device-width, initial-scal...</td>\n",
              "      <td></td>\n",
              "      <td>[http://stripe.rs-1198-a.com/stripe/beacon?cs_...</td>\n",
              "      <td>Californians Had Special Way to View the Eclip...</td>\n",
              "      <td>http://conservativebyte.com/2016/09/hillary-ba...</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>http://conservativebyte.com</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://usherald.com/wp-content/uploads/2015/05...</td>\n",
              "      <td>Thanks in part to the declassification of Defe...</td>\n",
              "      <td>[Bob Amoroso]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'generator': 'WordPress 4.8.1', 'og': {'site_...</td>\n",
              "      <td>http://usherald.com/breaking-declassified-docs...</td>\n",
              "      <td>[http://usherald.com/wp-content/uploads/2015/0...</td>\n",
              "      <td>Declassified Docs Show That Obama Admin Create...</td>\n",
              "      <td>http://usherald.com/breaking-declassified-docs...</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>http://usherald.com</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
              "      <td>Dolly Kyle has written a scathing “tell all” b...</td>\n",
              "      <td>[Fed Up]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'googlebot': 'noimageindex', 'generator': 'Po...</td>\n",
              "      <td>http://100percentfedup.com/hillary-on-disabled...</td>\n",
              "      <td>[https://www.facebook.com/tr?id=15790889156864...</td>\n",
              "      <td>HILLARY ON DISABLED CHILDREN During Easter Egg...</td>\n",
              "      <td>http://100percentfedup.com/hillary-on-disabled...</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>http://100percentfedup.com</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://freedomdaily.com/wp-content/uploads/201...</td>\n",
              "      <td>6.6k SHARES Facebook Twitter\\n\\nGerman Chancel...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'description': 'German chancellor Angela Merk...</td>\n",
              "      <td>http://freedomdaily.com/boom-merkel-admits-flo...</td>\n",
              "      <td>[http://1csabj4ddrd61fgqez2e4nss.wpengine.netd...</td>\n",
              "      <td>BOOM! Merkel Admits Flooding Germany With Musl...</td>\n",
              "      <td>http://freedomdaily.com/boom-merkel-admits-flo...</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>http://freedomdaily.com</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://occupydemocrats.com/wp-content/uploads/...</td>\n",
              "      <td>11.3k SHARES SHARE THIS STORY\\n\\nDuring a Repu...</td>\n",
              "      <td>[Grant Stern, Brett Bose, Natalie Dickinson]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'generator': 'Powered by Visual Composer - dr...</td>\n",
              "      <td>http://occupydemocrats.com/2016/09/23/just-new...</td>\n",
              "      <td>[http://occupydemocrats.com/wp-content/uploads...</td>\n",
              "      <td>Newsweek Accuses Trump Of Committing A Felony</td>\n",
              "      <td>http://occupydemocrats.com/2016/09/23/just-new...</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>http://occupydemocrats.com</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             top_img  \\\n",
              "0  http://conservativetribune.com/wp-content/uplo...   \n",
              "1  http://usherald.com/wp-content/uploads/2015/05...   \n",
              "2  http://100percentfedup.com/wp-content/uploads/...   \n",
              "3  http://freedomdaily.com/wp-content/uploads/201...   \n",
              "4  http://occupydemocrats.com/wp-content/uploads/...   \n",
              "\n",
              "                                                text  \\\n",
              "0  Olympic Committee Buckles to LGBT Groups... Ma...   \n",
              "1  Thanks in part to the declassification of Defe...   \n",
              "2  Dolly Kyle has written a scathing “tell all” b...   \n",
              "3  6.6k SHARES Facebook Twitter\\n\\nGerman Chancel...   \n",
              "4  11.3k SHARES SHARE THIS STORY\\n\\nDuring a Repu...   \n",
              "\n",
              "                                        authors keywords  \\\n",
              "0                                            []       []   \n",
              "1                                 [Bob Amoroso]       []   \n",
              "2                                      [Fed Up]       []   \n",
              "3                                            []       []   \n",
              "4  [Grant Stern, Brett Bose, Natalie Dickinson]       []   \n",
              "\n",
              "                                           meta_data  \\\n",
              "0  {'viewport': 'width=device-width, initial-scal...   \n",
              "1  {'generator': 'WordPress 4.8.1', 'og': {'site_...   \n",
              "2  {'googlebot': 'noimageindex', 'generator': 'Po...   \n",
              "3  {'description': 'German chancellor Angela Merk...   \n",
              "4  {'generator': 'Powered by Visual Composer - dr...   \n",
              "\n",
              "                                      canonical_link  \\\n",
              "0                                                      \n",
              "1  http://usherald.com/breaking-declassified-docs...   \n",
              "2  http://100percentfedup.com/hillary-on-disabled...   \n",
              "3  http://freedomdaily.com/boom-merkel-admits-flo...   \n",
              "4  http://occupydemocrats.com/2016/09/23/just-new...   \n",
              "\n",
              "                                              images  \\\n",
              "0  [http://stripe.rs-1198-a.com/stripe/beacon?cs_...   \n",
              "1  [http://usherald.com/wp-content/uploads/2015/0...   \n",
              "2  [https://www.facebook.com/tr?id=15790889156864...   \n",
              "3  [http://1csabj4ddrd61fgqez2e4nss.wpengine.netd...   \n",
              "4  [http://occupydemocrats.com/wp-content/uploads...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Californians Had Special Way to View the Eclip...   \n",
              "1  Declassified Docs Show That Obama Admin Create...   \n",
              "2  HILLARY ON DISABLED CHILDREN During Easter Egg...   \n",
              "3  BOOM! Merkel Admits Flooding Germany With Musl...   \n",
              "4      Newsweek Accuses Trump Of Committing A Felony   \n",
              "\n",
              "                                                 url summary  ...  \\\n",
              "0  http://conservativebyte.com/2016/09/hillary-ba...          ...   \n",
              "1  http://usherald.com/breaking-declassified-docs...          ...   \n",
              "2  http://100percentfedup.com/hillary-on-disabled...          ...   \n",
              "3  http://freedomdaily.com/boom-merkel-admits-flo...          ...   \n",
              "4  http://occupydemocrats.com/2016/09/23/just-new...          ...   \n",
              "\n",
              "                        source Real news_number num_shared shared_by_top  \\\n",
              "0  http://conservativebyte.com    0         NaN        NaN           NaN   \n",
              "1          http://usherald.com    0         NaN        NaN           NaN   \n",
              "2   http://100percentfedup.com    0         NaN        NaN           NaN   \n",
              "3      http://freedomdaily.com    0         NaN        NaN           NaN   \n",
              "4   http://occupydemocrats.com    0         NaN        NaN           NaN   \n",
              "\n",
              "  avg_follower avg_followee f_ratio betweenness_centrality  \\\n",
              "0          NaN          NaN     NaN                    NaN   \n",
              "1          NaN          NaN     NaN                    NaN   \n",
              "2          NaN          NaN     NaN                    NaN   \n",
              "3          NaN          NaN     NaN                    NaN   \n",
              "4          NaN          NaN     NaN                    NaN   \n",
              "\n",
              "  closenness_centrality  \n",
              "0                   NaN  \n",
              "1                   NaN  \n",
              "2                   NaN  \n",
              "3                   NaN  \n",
              "4                   NaN  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols = list(pf_news.columns)\n",
        "news = pd.DataFrame(columns=cols)\n",
        "news = pd.concat([news,bf_fake_news],axis=0,ignore_index=True)\n",
        "news = news.sample(frac=1).reset_index(drop=True)\n",
        "news.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0AcEleF7ckt"
      },
      "source": [
        "Because the four news content datasets have the same fields and do not conatin any information exclusive to the individual datasets, in contrast to the network data, we can merge them into a single dataset for use in our data mining algorithms. This has the positive effect of increasing our sample size to 422 total news stories. The ratio of true stories to false stories is 50:50. Note that we added a logical variable 'Real' to indicate whether the story is real or fake and the order of the instances has been randomized to eliminate patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV-KK3MrRAq8"
      },
      "source": [
        "## Feature Engineering: News Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EwuMlCxm9vT",
        "outputId": "519ea0ce-8c70-4ffd-9cd8-0c935d478323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/eviofekeze/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/eviofekeze/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "# from emoji import UNICODE_EMOJI\n",
        "from emoji import unicode_codes\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['🥇',\n",
              " '🥈',\n",
              " '🥉',\n",
              " '🆎',\n",
              " '🏧',\n",
              " '🅰️',\n",
              " '🅰',\n",
              " '🇦🇫',\n",
              " '🇦🇱',\n",
              " '🇩🇿',\n",
              " '🇦🇸',\n",
              " '🇦🇩',\n",
              " '🇦🇴',\n",
              " '🇦🇮',\n",
              " '🇦🇶',\n",
              " '🇦🇬',\n",
              " '♒',\n",
              " '🇦🇷',\n",
              " '♈',\n",
              " '🇦🇲',\n",
              " '🇦🇼',\n",
              " '🇦🇨',\n",
              " '🇦🇺',\n",
              " '🇦🇹',\n",
              " '🇦🇿',\n",
              " '🔙',\n",
              " '🅱️',\n",
              " '🅱',\n",
              " '🇧🇸',\n",
              " '🇧🇭',\n",
              " '🇧🇩',\n",
              " '🇧🇧',\n",
              " '🇧🇾',\n",
              " '🇧🇪',\n",
              " '🇧🇿',\n",
              " '🇧🇯',\n",
              " '🇧🇲',\n",
              " '🇧🇹',\n",
              " '🇧🇴',\n",
              " '🇧🇦',\n",
              " '🇧🇼',\n",
              " '🇧🇻',\n",
              " '🇧🇷',\n",
              " '🇮🇴',\n",
              " '🇻🇬',\n",
              " '🇧🇳',\n",
              " '🇧🇬',\n",
              " '🇧🇫',\n",
              " '🇧🇮',\n",
              " '🆑',\n",
              " '🆒',\n",
              " '🇰🇭',\n",
              " '🇨🇲',\n",
              " '🇨🇦',\n",
              " '🇮🇨',\n",
              " '♋',\n",
              " '🇨🇻',\n",
              " '♑',\n",
              " '🇧🇶',\n",
              " '🇰🇾',\n",
              " '🇨🇫',\n",
              " '🇪🇦',\n",
              " '🇹🇩',\n",
              " '🇨🇱',\n",
              " '🇨🇳',\n",
              " '🇨🇽',\n",
              " '🎄',\n",
              " '🇨🇵',\n",
              " '🇨🇨',\n",
              " '🇨🇴',\n",
              " '🇰🇲',\n",
              " '🇨🇬',\n",
              " '🇨🇩',\n",
              " '🇨🇰',\n",
              " '🇨🇷',\n",
              " '🇭🇷',\n",
              " '🇨🇺',\n",
              " '🇨🇼',\n",
              " '🇨🇾',\n",
              " '🇨🇿',\n",
              " '🇨🇮',\n",
              " '🇩🇰',\n",
              " '🇩🇬',\n",
              " '🇩🇯',\n",
              " '🇩🇲',\n",
              " '🇩🇴',\n",
              " '🔚',\n",
              " '🇪🇨',\n",
              " '🇪🇬',\n",
              " '🇸🇻',\n",
              " '🏴\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f',\n",
              " '🇬🇶',\n",
              " '🇪🇷',\n",
              " '🇪🇪',\n",
              " '🇸🇿',\n",
              " '🇪🇹',\n",
              " '🇪🇺',\n",
              " '🆓',\n",
              " '🇫🇰',\n",
              " '🇫🇴',\n",
              " '🇫🇯',\n",
              " '🇫🇮',\n",
              " '🇫🇷',\n",
              " '🇬🇫',\n",
              " '🇵🇫',\n",
              " '🇹🇫',\n",
              " '🇬🇦',\n",
              " '🇬🇲',\n",
              " '♊',\n",
              " '🇬🇪',\n",
              " '🇩🇪',\n",
              " '🇬🇭',\n",
              " '🇬🇮',\n",
              " '🇬🇷',\n",
              " '🇬🇱',\n",
              " '🇬🇩',\n",
              " '🇬🇵',\n",
              " '🇬🇺',\n",
              " '🇬🇹',\n",
              " '🇬🇬',\n",
              " '🇬🇳',\n",
              " '🇬🇼',\n",
              " '🇬🇾',\n",
              " '🇭🇹',\n",
              " '🇭🇲',\n",
              " '🇭🇳',\n",
              " '🇭🇰',\n",
              " '🇭🇺',\n",
              " '🆔',\n",
              " '🇮🇸',\n",
              " '🇮🇳',\n",
              " '🇮🇩',\n",
              " '🇮🇷',\n",
              " '🇮🇶',\n",
              " '🇮🇪',\n",
              " '🇮🇲',\n",
              " '🇮🇱',\n",
              " '🇮🇹',\n",
              " '🇯🇲',\n",
              " '🇯🇵',\n",
              " '🉑',\n",
              " '🈸',\n",
              " '🉐',\n",
              " '🏯',\n",
              " '㊗️',\n",
              " '㊗',\n",
              " '🈹',\n",
              " '🎎',\n",
              " '🈚',\n",
              " '🈁',\n",
              " '🈷️',\n",
              " '🈷',\n",
              " '🈵',\n",
              " '🈶',\n",
              " '🈺',\n",
              " '🈴',\n",
              " '🏣',\n",
              " '🈲',\n",
              " '🈯',\n",
              " '㊙️',\n",
              " '㊙',\n",
              " '🈂️',\n",
              " '🈂',\n",
              " '🔰',\n",
              " '🈳',\n",
              " '🇯🇪',\n",
              " '🇯🇴',\n",
              " '🇰🇿',\n",
              " '🇰🇪',\n",
              " '🇰🇮',\n",
              " '🇽🇰',\n",
              " '🇰🇼',\n",
              " '🇰🇬',\n",
              " '🇱🇦',\n",
              " '🇱🇻',\n",
              " '🇱🇧',\n",
              " '♌',\n",
              " '🇱🇸',\n",
              " '🇱🇷',\n",
              " '♎',\n",
              " '🇱🇾',\n",
              " '🇱🇮',\n",
              " '🇱🇹',\n",
              " '🇱🇺',\n",
              " '🇲🇴',\n",
              " '🇲🇬',\n",
              " '🇲🇼',\n",
              " '🇲🇾',\n",
              " '🇲🇻',\n",
              " '🇲🇱',\n",
              " '🇲🇹',\n",
              " '🇲🇭',\n",
              " '🇲🇶',\n",
              " '🇲🇷',\n",
              " '🇲🇺',\n",
              " '🇾🇹',\n",
              " '🇲🇽',\n",
              " '🇫🇲',\n",
              " '🇲🇩',\n",
              " '🇲🇨',\n",
              " '🇲🇳',\n",
              " '🇲🇪',\n",
              " '🇲🇸',\n",
              " '🇲🇦',\n",
              " '🇲🇿',\n",
              " '🤶',\n",
              " '🤶🏿',\n",
              " '🤶🏻',\n",
              " '🤶🏾',\n",
              " '🤶🏼',\n",
              " '🤶🏽',\n",
              " '🇲🇲',\n",
              " '🆕',\n",
              " '🆖',\n",
              " '🇳🇦',\n",
              " '🇳🇷',\n",
              " '🇳🇵',\n",
              " '🇳🇱',\n",
              " '🇳🇨',\n",
              " '🇳🇿',\n",
              " '🇳🇮',\n",
              " '🇳🇪',\n",
              " '🇳🇬',\n",
              " '🇳🇺',\n",
              " '🇳🇫',\n",
              " '🇰🇵',\n",
              " '🇲🇰',\n",
              " '🇲🇵',\n",
              " '🇳🇴',\n",
              " '🆗',\n",
              " '👌',\n",
              " '👌🏿',\n",
              " '👌🏻',\n",
              " '👌🏾',\n",
              " '👌🏼',\n",
              " '👌🏽',\n",
              " '🔛',\n",
              " '🅾️',\n",
              " '🅾',\n",
              " '🇴🇲',\n",
              " '⛎',\n",
              " '🅿️',\n",
              " '🅿',\n",
              " '🇵🇰',\n",
              " '🇵🇼',\n",
              " '🇵🇸',\n",
              " '🇵🇦',\n",
              " '🇵🇬',\n",
              " '🇵🇾',\n",
              " '🇵🇪',\n",
              " '🇵🇭',\n",
              " '♓',\n",
              " '🇵🇳',\n",
              " '🇵🇱',\n",
              " '🇵🇹',\n",
              " '🇵🇷',\n",
              " '🇶🇦',\n",
              " '🇷🇴',\n",
              " '🇷🇺',\n",
              " '🇷🇼',\n",
              " '🇷🇪',\n",
              " '🔜',\n",
              " '🆘',\n",
              " '♐',\n",
              " '🇼🇸',\n",
              " '🇸🇲',\n",
              " '🎅',\n",
              " '🎅🏿',\n",
              " '🎅🏻',\n",
              " '🎅🏾',\n",
              " '🎅🏼',\n",
              " '🎅🏽',\n",
              " '🇸🇦',\n",
              " '♏',\n",
              " '🏴\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f',\n",
              " '🇸🇳',\n",
              " '🇷🇸',\n",
              " '🇸🇨',\n",
              " '🇸🇱',\n",
              " '🇸🇬',\n",
              " '🇸🇽',\n",
              " '🇸🇰',\n",
              " '🇸🇮',\n",
              " '🇸🇧',\n",
              " '🇸🇴',\n",
              " '🇿🇦',\n",
              " '🇬🇸',\n",
              " '🇰🇷',\n",
              " '🇸🇸',\n",
              " '🇪🇸',\n",
              " '🇱🇰',\n",
              " '🇧🇱',\n",
              " '🇸🇭',\n",
              " '🇰🇳',\n",
              " '🇱🇨',\n",
              " '🇲🇫',\n",
              " '🇵🇲',\n",
              " '🇻🇨',\n",
              " '🗽',\n",
              " '🇸🇩',\n",
              " '🇸🇷',\n",
              " '🇸🇯',\n",
              " '🇸🇪',\n",
              " '🇨🇭',\n",
              " '🇸🇾',\n",
              " '🇸🇹',\n",
              " '🦖',\n",
              " '🔝',\n",
              " '🇹🇼',\n",
              " '🇹🇯',\n",
              " '🇹🇿',\n",
              " '♉',\n",
              " '🇹🇭',\n",
              " '🇹🇱',\n",
              " '🇹🇬',\n",
              " '🇹🇰',\n",
              " '🗼',\n",
              " '🇹🇴',\n",
              " '🇹🇹',\n",
              " '🇹🇦',\n",
              " '🇹🇳',\n",
              " '🇹🇷',\n",
              " '🇹🇲',\n",
              " '🇹🇨',\n",
              " '🇹🇻',\n",
              " '🇺🇲',\n",
              " '🇻🇮',\n",
              " '🆙',\n",
              " '🇺🇬',\n",
              " '🇺🇦',\n",
              " '🇦🇪',\n",
              " '🇬🇧',\n",
              " '🇺🇳',\n",
              " '🇺🇸',\n",
              " '🇺🇾',\n",
              " '🇺🇿',\n",
              " '🆚',\n",
              " '🇻🇺',\n",
              " '🇻🇦',\n",
              " '🇻🇪',\n",
              " '🇻🇳',\n",
              " '♍',\n",
              " '🏴\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f',\n",
              " '🇼🇫',\n",
              " '🇪🇭',\n",
              " '🇾🇪',\n",
              " '🇿🇲',\n",
              " '🇿🇼',\n",
              " '🧮',\n",
              " '🪗',\n",
              " '🩹',\n",
              " '🎟️',\n",
              " '🎟',\n",
              " '🚡',\n",
              " '✈️',\n",
              " '✈',\n",
              " '🛬',\n",
              " '🛫',\n",
              " '⏰',\n",
              " '⚗️',\n",
              " '⚗',\n",
              " '👽',\n",
              " '👾',\n",
              " '🚑',\n",
              " '🏈',\n",
              " '🏺',\n",
              " '🫀',\n",
              " '⚓',\n",
              " '💢',\n",
              " '😠',\n",
              " '👿',\n",
              " '😧',\n",
              " '🐜',\n",
              " '📶',\n",
              " '😰',\n",
              " '🚛',\n",
              " '🧑\\u200d🎨',\n",
              " '🧑🏿\\u200d🎨',\n",
              " '🧑🏻\\u200d🎨',\n",
              " '🧑🏾\\u200d🎨',\n",
              " '🧑🏼\\u200d🎨',\n",
              " '🧑🏽\\u200d🎨',\n",
              " '🎨',\n",
              " '😲',\n",
              " '🧑\\u200d🚀',\n",
              " '🧑🏿\\u200d🚀',\n",
              " '🧑🏻\\u200d🚀',\n",
              " '🧑🏾\\u200d🚀',\n",
              " '🧑🏼\\u200d🚀',\n",
              " '🧑🏽\\u200d🚀',\n",
              " '⚛️',\n",
              " '⚛',\n",
              " '🛺',\n",
              " '🚗',\n",
              " '🥑',\n",
              " '🪓',\n",
              " '👶',\n",
              " '👼',\n",
              " '👼🏿',\n",
              " '👼🏻',\n",
              " '👼🏾',\n",
              " '👼🏼',\n",
              " '👼🏽',\n",
              " '🍼',\n",
              " '🐤',\n",
              " '👶🏿',\n",
              " '👶🏻',\n",
              " '👶🏾',\n",
              " '👶🏼',\n",
              " '👶🏽',\n",
              " '🚼',\n",
              " '👇',\n",
              " '👇🏿',\n",
              " '👇🏻',\n",
              " '👇🏾',\n",
              " '👇🏼',\n",
              " '👇🏽',\n",
              " '👈',\n",
              " '👈🏿',\n",
              " '👈🏻',\n",
              " '👈🏾',\n",
              " '👈🏼',\n",
              " '👈🏽',\n",
              " '👉',\n",
              " '👉🏿',\n",
              " '👉🏻',\n",
              " '👉🏾',\n",
              " '👉🏼',\n",
              " '👉🏽',\n",
              " '👆',\n",
              " '👆🏿',\n",
              " '👆🏻',\n",
              " '👆🏾',\n",
              " '👆🏼',\n",
              " '👆🏽',\n",
              " '🎒',\n",
              " '🥓',\n",
              " '🦡',\n",
              " '🏸',\n",
              " '🥯',\n",
              " '🛄',\n",
              " '🥖',\n",
              " '⚖️',\n",
              " '⚖',\n",
              " '🦲',\n",
              " '🩰',\n",
              " '🎈',\n",
              " '🗳️',\n",
              " '🗳',\n",
              " '🍌',\n",
              " '🪕',\n",
              " '🏦',\n",
              " '📊',\n",
              " '💈',\n",
              " '⚾',\n",
              " '🧺',\n",
              " '🏀',\n",
              " '🦇',\n",
              " '🛁',\n",
              " '🔋',\n",
              " '🏖️',\n",
              " '🏖',\n",
              " '😁',\n",
              " '\\U0001fad8',\n",
              " '🐻',\n",
              " '💓',\n",
              " '🦫',\n",
              " '🛏️',\n",
              " '🛏',\n",
              " '🍺',\n",
              " '🪲',\n",
              " '🔔',\n",
              " '🫑',\n",
              " '🔕',\n",
              " '🛎️',\n",
              " '🛎',\n",
              " '🍱',\n",
              " '🧃',\n",
              " '🚲',\n",
              " '👙',\n",
              " '🧢',\n",
              " '☣️',\n",
              " '☣',\n",
              " '🐦',\n",
              " '🎂',\n",
              " '🦬',\n",
              " '\\U0001fae6',\n",
              " '🐈\\u200d⬛',\n",
              " '⚫',\n",
              " '🏴',\n",
              " '🖤',\n",
              " '⬛',\n",
              " '◾',\n",
              " '◼️',\n",
              " '◼',\n",
              " '✒️',\n",
              " '✒',\n",
              " '▪️',\n",
              " '▪',\n",
              " '🔲',\n",
              " '🌼',\n",
              " '🐡',\n",
              " '📘',\n",
              " '🔵',\n",
              " '💙',\n",
              " '🟦',\n",
              " '🫐',\n",
              " '🐗',\n",
              " '💣',\n",
              " '🦴',\n",
              " '🔖',\n",
              " '📑',\n",
              " '📚',\n",
              " '🪃',\n",
              " '🍾',\n",
              " '💐',\n",
              " '🏹',\n",
              " '🥣',\n",
              " '🎳',\n",
              " '🥊',\n",
              " '👦',\n",
              " '👦🏿',\n",
              " '👦🏻',\n",
              " '👦🏾',\n",
              " '👦🏼',\n",
              " '👦🏽',\n",
              " '🧠',\n",
              " '🍞',\n",
              " '🤱',\n",
              " '🤱🏿',\n",
              " '🤱🏻',\n",
              " '🤱🏾',\n",
              " '🤱🏼',\n",
              " '🤱🏽',\n",
              " '🧱',\n",
              " '🌉',\n",
              " '💼',\n",
              " '🩲',\n",
              " '🔆',\n",
              " '🥦',\n",
              " '💔',\n",
              " '🧹',\n",
              " '🟤',\n",
              " '🤎',\n",
              " '🟫',\n",
              " '🧋',\n",
              " '\\U0001fae7',\n",
              " '🪣',\n",
              " '🐛',\n",
              " '🏗️',\n",
              " '🏗',\n",
              " '🚅',\n",
              " '🎯',\n",
              " '🌯',\n",
              " '🚌',\n",
              " '🚏',\n",
              " '👤',\n",
              " '👥',\n",
              " '🧈',\n",
              " '🦋',\n",
              " '🌵',\n",
              " '📅',\n",
              " '🤙',\n",
              " '🤙🏿',\n",
              " '🤙🏻',\n",
              " '🤙🏾',\n",
              " '🤙🏼',\n",
              " '🤙🏽',\n",
              " '🐪',\n",
              " '📷',\n",
              " '📸',\n",
              " '🏕️',\n",
              " '🏕',\n",
              " '🕯️',\n",
              " '🕯',\n",
              " '🍬',\n",
              " '🥫',\n",
              " '🛶',\n",
              " '🗃️',\n",
              " '🗃',\n",
              " '📇',\n",
              " '🗂️',\n",
              " '🗂',\n",
              " '🎠',\n",
              " '🎏',\n",
              " '🪚',\n",
              " '🥕',\n",
              " '🏰',\n",
              " '🐈',\n",
              " '🐱',\n",
              " '😹',\n",
              " '😼',\n",
              " '⛓️',\n",
              " '⛓',\n",
              " '🪑',\n",
              " '📉',\n",
              " '📈',\n",
              " '💹',\n",
              " '☑️',\n",
              " '☑',\n",
              " '✔️',\n",
              " '✔',\n",
              " '✅',\n",
              " '🧀',\n",
              " '🏁',\n",
              " '🍒',\n",
              " '🌸',\n",
              " '♟️',\n",
              " '♟',\n",
              " '🌰',\n",
              " '🐔',\n",
              " '🧒',\n",
              " '🧒🏿',\n",
              " '🧒🏻',\n",
              " '🧒🏾',\n",
              " '🧒🏼',\n",
              " '🧒🏽',\n",
              " '🚸',\n",
              " '🐿️',\n",
              " '🐿',\n",
              " '🍫',\n",
              " '🥢',\n",
              " '⛪',\n",
              " '🚬',\n",
              " '🎦',\n",
              " 'Ⓜ️',\n",
              " 'Ⓜ',\n",
              " '🎪',\n",
              " '🏙️',\n",
              " '🏙',\n",
              " '🌆',\n",
              " '🗜️',\n",
              " '🗜',\n",
              " '🎬',\n",
              " '👏',\n",
              " '👏🏿',\n",
              " '👏🏻',\n",
              " '👏🏾',\n",
              " '👏🏼',\n",
              " '👏🏽',\n",
              " '🏛️',\n",
              " '🏛',\n",
              " '🍻',\n",
              " '🥂',\n",
              " '📋',\n",
              " '🔃',\n",
              " '📕',\n",
              " '📪',\n",
              " '📫',\n",
              " '🌂',\n",
              " '☁️',\n",
              " '☁',\n",
              " '🌩️',\n",
              " '🌩',\n",
              " '⛈️',\n",
              " '⛈',\n",
              " '🌧️',\n",
              " '🌧',\n",
              " '🌨️',\n",
              " '🌨',\n",
              " '🤡',\n",
              " '♣️',\n",
              " '♣',\n",
              " '👝',\n",
              " '🧥',\n",
              " '🪳',\n",
              " '🍸',\n",
              " '🥥',\n",
              " '⚰️',\n",
              " '⚰',\n",
              " '🪙',\n",
              " '🥶',\n",
              " '💥',\n",
              " '☄️',\n",
              " '☄',\n",
              " '🧭',\n",
              " '💽',\n",
              " '🖱️',\n",
              " '🖱',\n",
              " '🎊',\n",
              " '😖',\n",
              " '😕',\n",
              " '🚧',\n",
              " '👷',\n",
              " '👷🏿',\n",
              " '👷🏻',\n",
              " '👷🏾',\n",
              " '👷🏼',\n",
              " '👷🏽',\n",
              " '🎛️',\n",
              " '🎛',\n",
              " '🏪',\n",
              " '🧑\\u200d🍳',\n",
              " '🧑🏿\\u200d🍳',\n",
              " '🧑🏻\\u200d🍳',\n",
              " '🧑🏾\\u200d🍳',\n",
              " '🧑🏼\\u200d🍳',\n",
              " '🧑🏽\\u200d🍳',\n",
              " '🍚',\n",
              " '🍪',\n",
              " '🍳',\n",
              " '©️',\n",
              " '©',\n",
              " '\\U0001fab8',\n",
              " '🛋️',\n",
              " '🛋',\n",
              " '🔄',\n",
              " '💑',\n",
              " '💑🏿',\n",
              " '💑🏻',\n",
              " '👨\\u200d❤️\\u200d👨',\n",
              " '👨\\u200d❤\\u200d👨',\n",
              " '👨🏿\\u200d❤️\\u200d👨🏿',\n",
              " '👨🏿\\u200d❤\\u200d👨🏿',\n",
              " '👨🏿\\u200d❤️\\u200d👨🏻',\n",
              " '👨🏿\\u200d❤\\u200d👨🏻',\n",
              " '👨🏿\\u200d❤️\\u200d👨🏾',\n",
              " '👨🏿\\u200d❤\\u200d👨🏾',\n",
              " '👨🏿\\u200d❤️\\u200d👨🏼',\n",
              " '👨🏿\\u200d❤\\u200d👨🏼',\n",
              " '👨🏿\\u200d❤️\\u200d👨🏽',\n",
              " '👨🏿\\u200d❤\\u200d👨🏽',\n",
              " '👨🏻\\u200d❤️\\u200d👨🏻',\n",
              " '👨🏻\\u200d❤\\u200d👨🏻',\n",
              " '👨🏻\\u200d❤️\\u200d👨🏿',\n",
              " '👨🏻\\u200d❤\\u200d👨🏿',\n",
              " '👨🏻\\u200d❤️\\u200d👨🏾',\n",
              " '👨🏻\\u200d❤\\u200d👨🏾',\n",
              " '👨🏻\\u200d❤️\\u200d👨🏼',\n",
              " '👨🏻\\u200d❤\\u200d👨🏼',\n",
              " '👨🏻\\u200d❤️\\u200d👨🏽',\n",
              " '👨🏻\\u200d❤\\u200d👨🏽',\n",
              " '👨🏾\\u200d❤️\\u200d👨🏾',\n",
              " '👨🏾\\u200d❤\\u200d👨🏾',\n",
              " '👨🏾\\u200d❤️\\u200d👨🏿',\n",
              " '👨🏾\\u200d❤\\u200d👨🏿',\n",
              " '👨🏾\\u200d❤️\\u200d👨🏻',\n",
              " '👨🏾\\u200d❤\\u200d👨🏻',\n",
              " '👨🏾\\u200d❤️\\u200d👨🏼',\n",
              " '👨🏾\\u200d❤\\u200d👨🏼',\n",
              " '👨🏾\\u200d❤️\\u200d👨🏽',\n",
              " '👨🏾\\u200d❤\\u200d👨🏽',\n",
              " '👨🏼\\u200d❤️\\u200d👨🏼',\n",
              " '👨🏼\\u200d❤\\u200d👨🏼',\n",
              " '👨🏼\\u200d❤️\\u200d👨🏿',\n",
              " '👨🏼\\u200d❤\\u200d👨🏿',\n",
              " '👨🏼\\u200d❤️\\u200d👨🏻',\n",
              " '👨🏼\\u200d❤\\u200d👨🏻',\n",
              " '👨🏼\\u200d❤️\\u200d👨🏾',\n",
              " '👨🏼\\u200d❤\\u200d👨🏾',\n",
              " '👨🏼\\u200d❤️\\u200d👨🏽',\n",
              " '👨🏼\\u200d❤\\u200d👨🏽',\n",
              " '👨🏽\\u200d❤️\\u200d👨🏽',\n",
              " '👨🏽\\u200d❤\\u200d👨🏽',\n",
              " '👨🏽\\u200d❤️\\u200d👨🏿',\n",
              " '👨🏽\\u200d❤\\u200d👨🏿',\n",
              " '👨🏽\\u200d❤️\\u200d👨🏻',\n",
              " '👨🏽\\u200d❤\\u200d👨🏻',\n",
              " '👨🏽\\u200d❤️\\u200d👨🏾',\n",
              " '👨🏽\\u200d❤\\u200d👨🏾',\n",
              " '👨🏽\\u200d❤️\\u200d👨🏼',\n",
              " '👨🏽\\u200d❤\\u200d👨🏼',\n",
              " '💑🏾',\n",
              " '💑🏼',\n",
              " '💑🏽',\n",
              " '🧑🏿\\u200d❤️\\u200d🧑🏻',\n",
              " '🧑🏿\\u200d❤\\u200d🧑🏻',\n",
              " '🧑🏿\\u200d❤️\\u200d🧑🏾',\n",
              " '🧑🏿\\u200d❤\\u200d🧑🏾',\n",
              " '🧑🏿\\u200d❤️\\u200d🧑🏼',\n",
              " '🧑🏿\\u200d❤\\u200d🧑🏼',\n",
              " '🧑🏿\\u200d❤️\\u200d🧑🏽',\n",
              " '🧑🏿\\u200d❤\\u200d🧑🏽',\n",
              " '🧑🏻\\u200d❤️\\u200d🧑🏿',\n",
              " '🧑🏻\\u200d❤\\u200d🧑🏿',\n",
              " '🧑🏻\\u200d❤️\\u200d🧑🏾',\n",
              " '🧑🏻\\u200d❤\\u200d🧑🏾',\n",
              " '🧑🏻\\u200d❤️\\u200d🧑🏼',\n",
              " '🧑🏻\\u200d❤\\u200d🧑🏼',\n",
              " '🧑🏻\\u200d❤️\\u200d🧑🏽',\n",
              " '🧑🏻\\u200d❤\\u200d🧑🏽',\n",
              " '🧑🏾\\u200d❤️\\u200d🧑🏿',\n",
              " '🧑🏾\\u200d❤\\u200d🧑🏿',\n",
              " '🧑🏾\\u200d❤️\\u200d🧑🏻',\n",
              " '🧑🏾\\u200d❤\\u200d🧑🏻',\n",
              " '🧑🏾\\u200d❤️\\u200d🧑🏼',\n",
              " '🧑🏾\\u200d❤\\u200d🧑🏼',\n",
              " '🧑🏾\\u200d❤️\\u200d🧑🏽',\n",
              " '🧑🏾\\u200d❤\\u200d🧑🏽',\n",
              " '🧑🏼\\u200d❤️\\u200d🧑🏿',\n",
              " '🧑🏼\\u200d❤\\u200d🧑🏿',\n",
              " '🧑🏼\\u200d❤️\\u200d🧑🏻',\n",
              " '🧑🏼\\u200d❤\\u200d🧑🏻',\n",
              " '🧑🏼\\u200d❤️\\u200d🧑🏾',\n",
              " '🧑🏼\\u200d❤\\u200d🧑🏾',\n",
              " '🧑🏼\\u200d❤️\\u200d🧑🏽',\n",
              " '🧑🏼\\u200d❤\\u200d🧑🏽',\n",
              " '🧑🏽\\u200d❤️\\u200d🧑🏿',\n",
              " '🧑🏽\\u200d❤\\u200d🧑🏿',\n",
              " '🧑🏽\\u200d❤️\\u200d🧑🏻',\n",
              " '🧑🏽\\u200d❤\\u200d🧑🏻',\n",
              " '🧑🏽\\u200d❤️\\u200d🧑🏾',\n",
              " '🧑🏽\\u200d❤\\u200d🧑🏾',\n",
              " '🧑🏽\\u200d❤️\\u200d🧑🏼',\n",
              " '🧑🏽\\u200d❤\\u200d🧑🏼',\n",
              " '👩\\u200d❤️\\u200d👨',\n",
              " '👩\\u200d❤\\u200d👨',\n",
              " '👩🏿\\u200d❤️\\u200d👨🏿',\n",
              " '👩🏿\\u200d❤\\u200d👨🏿',\n",
              " '👩🏿\\u200d❤️\\u200d👨🏻',\n",
              " '👩🏿\\u200d❤\\u200d👨🏻',\n",
              " '👩🏿\\u200d❤️\\u200d👨🏾',\n",
              " '👩🏿\\u200d❤\\u200d👨🏾',\n",
              " '👩🏿\\u200d❤️\\u200d👨🏼',\n",
              " '👩🏿\\u200d❤\\u200d👨🏼',\n",
              " '👩🏿\\u200d❤️\\u200d👨🏽',\n",
              " '👩🏿\\u200d❤\\u200d👨🏽',\n",
              " '👩🏻\\u200d❤️\\u200d👨🏻',\n",
              " '👩🏻\\u200d❤\\u200d👨🏻',\n",
              " '👩🏻\\u200d❤️\\u200d👨🏿',\n",
              " '👩🏻\\u200d❤\\u200d👨🏿',\n",
              " '👩🏻\\u200d❤️\\u200d👨🏾',\n",
              " '👩🏻\\u200d❤\\u200d👨🏾',\n",
              " '👩🏻\\u200d❤️\\u200d👨🏼',\n",
              " '👩🏻\\u200d❤\\u200d👨🏼',\n",
              " '👩🏻\\u200d❤️\\u200d👨🏽',\n",
              " '👩🏻\\u200d❤\\u200d👨🏽',\n",
              " '👩🏾\\u200d❤️\\u200d👨🏾',\n",
              " '👩🏾\\u200d❤\\u200d👨🏾',\n",
              " '👩🏾\\u200d❤️\\u200d👨🏿',\n",
              " '👩🏾\\u200d❤\\u200d👨🏿',\n",
              " '👩🏾\\u200d❤️\\u200d👨🏻',\n",
              " '👩🏾\\u200d❤\\u200d👨🏻',\n",
              " '👩🏾\\u200d❤️\\u200d👨🏼',\n",
              " '👩🏾\\u200d❤\\u200d👨🏼',\n",
              " '👩🏾\\u200d❤️\\u200d👨🏽',\n",
              " '👩🏾\\u200d❤\\u200d👨🏽',\n",
              " '👩🏼\\u200d❤️\\u200d👨🏼',\n",
              " '👩🏼\\u200d❤\\u200d👨🏼',\n",
              " '👩🏼\\u200d❤️\\u200d👨🏿',\n",
              " '👩🏼\\u200d❤\\u200d👨🏿',\n",
              " '👩🏼\\u200d❤️\\u200d👨🏻',\n",
              " '👩🏼\\u200d❤\\u200d👨🏻',\n",
              " '👩🏼\\u200d❤️\\u200d👨🏾',\n",
              " '👩🏼\\u200d❤\\u200d👨🏾',\n",
              " '👩🏼\\u200d❤️\\u200d👨🏽',\n",
              " '👩🏼\\u200d❤\\u200d👨🏽',\n",
              " '👩🏽\\u200d❤️\\u200d👨🏽',\n",
              " '👩🏽\\u200d❤\\u200d👨🏽',\n",
              " '👩🏽\\u200d❤️\\u200d👨🏿',\n",
              " '👩🏽\\u200d❤\\u200d👨🏿',\n",
              " '👩🏽\\u200d❤️\\u200d👨🏻',\n",
              " '👩🏽\\u200d❤\\u200d👨🏻',\n",
              " '👩🏽\\u200d❤️\\u200d👨🏾',\n",
              " '👩🏽\\u200d❤\\u200d👨🏾',\n",
              " '👩🏽\\u200d❤️\\u200d👨🏼',\n",
              " '👩🏽\\u200d❤\\u200d👨🏼',\n",
              " '👩\\u200d❤️\\u200d👩',\n",
              " '👩\\u200d❤\\u200d👩',\n",
              " '👩🏿\\u200d❤️\\u200d👩🏿',\n",
              " '👩🏿\\u200d❤\\u200d👩🏿',\n",
              " '👩🏿\\u200d❤️\\u200d👩🏻',\n",
              " '👩🏿\\u200d❤\\u200d👩🏻',\n",
              " '👩🏿\\u200d❤️\\u200d👩🏾',\n",
              " '👩🏿\\u200d❤\\u200d👩🏾',\n",
              " '👩🏿\\u200d❤️\\u200d👩🏼',\n",
              " '👩🏿\\u200d❤\\u200d👩🏼',\n",
              " '👩🏿\\u200d❤️\\u200d👩🏽',\n",
              " '👩🏿\\u200d❤\\u200d👩🏽',\n",
              " '👩🏻\\u200d❤️\\u200d👩🏻',\n",
              " '👩🏻\\u200d❤\\u200d👩🏻',\n",
              " '👩🏻\\u200d❤️\\u200d👩🏿',\n",
              " '👩🏻\\u200d❤\\u200d👩🏿',\n",
              " '👩🏻\\u200d❤️\\u200d👩🏾',\n",
              " '👩🏻\\u200d❤\\u200d👩🏾',\n",
              " '👩🏻\\u200d❤️\\u200d👩🏼',\n",
              " '👩🏻\\u200d❤\\u200d👩🏼',\n",
              " '👩🏻\\u200d❤️\\u200d👩🏽',\n",
              " '👩🏻\\u200d❤\\u200d👩🏽',\n",
              " '👩🏾\\u200d❤️\\u200d👩🏾',\n",
              " '👩🏾\\u200d❤\\u200d👩🏾',\n",
              " '👩🏾\\u200d❤️\\u200d👩🏿',\n",
              " '👩🏾\\u200d❤\\u200d👩🏿',\n",
              " '👩🏾\\u200d❤️\\u200d👩🏻',\n",
              " '👩🏾\\u200d❤\\u200d👩🏻',\n",
              " '👩🏾\\u200d❤️\\u200d👩🏼',\n",
              " '👩🏾\\u200d❤\\u200d👩🏼',\n",
              " '👩🏾\\u200d❤️\\u200d👩🏽',\n",
              " '👩🏾\\u200d❤\\u200d👩🏽',\n",
              " '👩🏼\\u200d❤️\\u200d👩🏼',\n",
              " '👩🏼\\u200d❤\\u200d👩🏼',\n",
              " '👩🏼\\u200d❤️\\u200d👩🏿',\n",
              " '👩🏼\\u200d❤\\u200d👩🏿',\n",
              " '👩🏼\\u200d❤️\\u200d👩🏻',\n",
              " '👩🏼\\u200d❤\\u200d👩🏻',\n",
              " '👩🏼\\u200d❤️\\u200d👩🏾',\n",
              " '👩🏼\\u200d❤\\u200d👩🏾',\n",
              " '👩🏼\\u200d❤️\\u200d👩🏽',\n",
              " '👩🏼\\u200d❤\\u200d👩🏽',\n",
              " '👩🏽\\u200d❤️\\u200d👩🏽',\n",
              " '👩🏽\\u200d❤\\u200d👩🏽',\n",
              " '👩🏽\\u200d❤️\\u200d👩🏿',\n",
              " '👩🏽\\u200d❤\\u200d👩🏿',\n",
              " '👩🏽\\u200d❤️\\u200d👩🏻',\n",
              " '👩🏽\\u200d❤\\u200d👩🏻',\n",
              " '👩🏽\\u200d❤️\\u200d👩🏾',\n",
              " '👩🏽\\u200d❤\\u200d👩🏾',\n",
              " '👩🏽\\u200d❤️\\u200d👩🏼',\n",
              " '👩🏽\\u200d❤\\u200d👩🏼',\n",
              " '🐄',\n",
              " '🐮',\n",
              " '🤠',\n",
              " '🦀',\n",
              " '🖍️',\n",
              " '🖍',\n",
              " '💳',\n",
              " '🌙',\n",
              " '🦗',\n",
              " '🏏',\n",
              " '🐊',\n",
              " '🥐',\n",
              " '❌',\n",
              " '❎',\n",
              " '🤞',\n",
              " '🤞🏿',\n",
              " '🤞🏻',\n",
              " '🤞🏾',\n",
              " '🤞🏼',\n",
              " '🤞🏽',\n",
              " '🎌',\n",
              " '⚔️',\n",
              " '⚔',\n",
              " '👑',\n",
              " '\\U0001fa7c',\n",
              " '😿',\n",
              " '😢',\n",
              " '🔮',\n",
              " '🥒',\n",
              " '🥤',\n",
              " '🧁',\n",
              " '🥌',\n",
              " '🦱',\n",
              " '➰',\n",
              " '💱',\n",
              " '🍛',\n",
              " '🍮',\n",
              " '🛃',\n",
              " '🥩',\n",
              " '🌀',\n",
              " '🗡️',\n",
              " '🗡',\n",
              " '🍡',\n",
              " '🏿',\n",
              " '💨',\n",
              " '🧏\\u200d♂️',\n",
              " '🧏\\u200d♂',\n",
              " '🧏🏿\\u200d♂️',\n",
              " '🧏🏿\\u200d♂',\n",
              " '🧏🏻\\u200d♂️',\n",
              " '🧏🏻\\u200d♂',\n",
              " '🧏🏾\\u200d♂️',\n",
              " '🧏🏾\\u200d♂',\n",
              " '🧏🏼\\u200d♂️',\n",
              " '🧏🏼\\u200d♂',\n",
              " '🧏🏽\\u200d♂️',\n",
              " '🧏🏽\\u200d♂',\n",
              " '🧏',\n",
              " '🧏🏿',\n",
              " '🧏🏻',\n",
              " '🧏🏾',\n",
              " '🧏🏼',\n",
              " '🧏🏽',\n",
              " '🧏\\u200d♀️',\n",
              " '🧏\\u200d♀',\n",
              " '🧏🏿\\u200d♀️',\n",
              " '🧏🏿\\u200d♀',\n",
              " '🧏🏻\\u200d♀️',\n",
              " '🧏🏻\\u200d♀',\n",
              " '🧏🏾\\u200d♀️',\n",
              " '🧏🏾\\u200d♀',\n",
              " '🧏🏼\\u200d♀️',\n",
              " '🧏🏼\\u200d♀',\n",
              " '🧏🏽\\u200d♀️',\n",
              " '🧏🏽\\u200d♀',\n",
              " '🌳',\n",
              " '🦌',\n",
              " '🚚',\n",
              " '🏬',\n",
              " '🏚️',\n",
              " '🏚',\n",
              " '🏜️',\n",
              " '🏜',\n",
              " '🏝️',\n",
              " '🏝',\n",
              " '🖥️',\n",
              " '🖥',\n",
              " '🕵️',\n",
              " '🕵',\n",
              " '🕵🏿',\n",
              " '🕵🏻',\n",
              " ...]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(emoji.unicode_codes.EMOJI_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "RUgA6dFMKiMR"
      },
      "outputs": [],
      "source": [
        "def num_all_caps(string):\n",
        "    if len(re.findall(r\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",string)) > 0:\n",
        "      return 0\n",
        "    return 1\n",
        "    # return len(re.findall(r\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",string))\n",
        "def num_exmarks(string):\n",
        "    if string.count(\"!\") > 0:\n",
        "      return 0\n",
        "    return 1\n",
        "    # return string.count(\"!\") \n",
        "\n",
        "def num_all_caps_or_exmarks(string):\n",
        "    if len(re.findall(r\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",string)) > 0:\n",
        "      return 0\n",
        "    if string.count(\"!\") > 0:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "def title_ln(text):\n",
        "  totalln = 0 \n",
        "  for character in word_tokenize(text):\n",
        "      totalln +=1\n",
        "  if totalln > 11:\n",
        "    return 0\n",
        "  return 1\n",
        "  # return totalln\n",
        "def text_ln(text):\n",
        "  totalln = 0 \n",
        "  for character in word_tokenize(text):\n",
        "      totalln +=1\n",
        "  if totalln > 500:\n",
        "    return 1\n",
        "  return 0\n",
        "  # return totalln\n",
        "def text_has_emoji(text):\n",
        "    for character in text:\n",
        "        if character in list(emoji.unicode_codes.EMOJI_DATA):\n",
        "            return 1\n",
        "    return 0\n",
        "def text_word_len(text):\n",
        "  longln = 0\n",
        "  totalln = 0 \n",
        "  for character in word_tokenize(text):\n",
        "    if character not in stop_words:\n",
        "      if len(character) > 6:\n",
        "        longln += 1\n",
        "      totalln +=1\n",
        "  if totalln != 0:\n",
        "    if longln / totalln > 0.3:\n",
        "      return 0\n",
        "  return 1\n",
        "  # if totalln != 0:\n",
        "  #   return longln / totalln\n",
        "  # return 0\n",
        "def stop_word_title(text):\n",
        "  stopln = 0\n",
        "  totalln = 0 \n",
        "  for character in word_tokenize(text):\n",
        "    if character in stop_words:\n",
        "        stopln += 1\n",
        "    totalln +=1\n",
        "  if totalln != 0:\n",
        "    if stopln / totalln > 0.14:\n",
        "      return 1\n",
        "  return 0\n",
        "  # if totalln != 0:\n",
        "  #   return stopln / totalln\n",
        "  # return 0\n",
        "def stop_word_text(text):\n",
        "  stopln = 0\n",
        "  totalln = 0 \n",
        "  for character in word_tokenize(text):\n",
        "    if character in stop_words:\n",
        "        stopln += 1\n",
        "    totalln +=1\n",
        "  if totalln != 0:\n",
        "    if stopln / totalln > 0.37:\n",
        "      return 1\n",
        "  return 0\n",
        "  # if totalln != 0:\n",
        "  #   return stopln / totalln\n",
        "  # return 0\n",
        "news['title_allcaps']=news['title'].apply(num_all_caps)\n",
        "news['title_num_exmarks'] = news['title'].apply(num_exmarks)\n",
        "news['title_allcaps_or_exmarks']=news['title'].apply(num_all_caps_or_exmarks)\n",
        "news['title_length']=news['title'].apply(title_ln)\n",
        "news['text_length']=news['text'].apply(text_ln)\n",
        "news['title_isascii']=news['title'].apply(text_has_emoji)\n",
        "news['text_isascii']=news['text'].apply(text_has_emoji)\n",
        "news['title_comp']=news['title'].apply(text_word_len)\n",
        "news['text_comp']=news['text'].apply(text_word_len)\n",
        "news['title_stopwords']=news['title'].apply(stop_word_title)\n",
        "news['text_stopwords']=news['text'].apply(stop_word_text)\n",
        "# Readability Scores\n",
        "# news['flesch_score'] =news['text'].apply(ts.flesch_reading_ease).apply(lambda x: x**2)\n",
        "news['flesch_score'] =news['text'].apply(ts.flesch_reading_ease)\n",
        "news['dale_chall_score'] =news['text'].apply(ts.dale_chall_readability_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK3LYP23RLuF"
      },
      "source": [
        "## Prepare Data Set for Data Mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbAXVqKMOINe"
      },
      "source": [
        "### Select Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGgLVU7MCpYA",
        "outputId": "17773ee5-c1ab-4642-bb12-fb35354abb58"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb Cell 61\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m feature_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtitle_isascii\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtitle_allcaps\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtitle_num_exmarks\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtitle_length\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mtext_length\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mflesch_score\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdale_chall_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle_comp\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mtext_comp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle_stopwords\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtext_stopwords\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_shared\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mshared_by_top\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg_follower\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mavg_followee\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mf_ratio\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mbetweenness_centrality\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mclosenness_centrality\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mReal\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtitle_allcaps_or_exmarks\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m news_ft \u001b[39m=\u001b[39m news[feature_list]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m news_ft[\u001b[39m'\u001b[39m\u001b[39mnum_shared\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mnews_ft[\u001b[39m'\u001b[39;49m\u001b[39mnum_shared\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m news_ft[\u001b[39m'\u001b[39m\u001b[39mReal\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mnews_ft[\u001b[39m'\u001b[39m\u001b[39mReal\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y113sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m news_ft\u001b[39m.\u001b[39minfo()\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/generic.py:5912\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5905\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   5906\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   5907\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   5908\u001b[0m     ]\n\u001b[1;32m   5910\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5911\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 5912\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   5913\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5915\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/internals/managers.py:419\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/internals/managers.py:304\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/internals/blocks.py:580\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 580\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    582\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    583\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1292\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m   1291\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1292\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m   1293\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m   1294\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1237\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1237\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m   1239\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1154\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m   1151\u001b[0m \n\u001b[1;32m   1152\u001b[0m     \u001b[39m# work around NumPy brokenness, #1987\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(dtype\u001b[39m.\u001b[39mtype, np\u001b[39m.\u001b[39minteger):\n\u001b[0;32m-> 1154\u001b[0m         \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mastype_intsafe(arr, dtype)\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[39m# then coerce to a proper dtype and recall astype_nansafe\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[39melif\u001b[39;00m is_datetime64_dtype(dtype):\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/pandas/_libs/lib.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
          ]
        }
      ],
      "source": [
        "feature_list = ['title_isascii','title_allcaps','title_num_exmarks','title_length',\n",
        "               'text_length','flesch_score','dale_chall_score', 'title_comp', \n",
        "                'text_comp', 'title_stopwords', 'text_stopwords', 'num_shared',\n",
        "               'shared_by_top','avg_follower', 'avg_followee', 'f_ratio',\n",
        "                'betweenness_centrality','closenness_centrality','Real',\"title_allcaps_or_exmarks\"]\n",
        "news_ft = news[feature_list].copy()\n",
        "news_ft['num_shared']=news_ft['num_shared'].astype('int')\n",
        "news_ft['Real']=news_ft['Real'].astype('int')\n",
        "news_ft.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4bCd-w1NKek"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zCcWSGmTpPi8"
      },
      "outputs": [],
      "source": [
        "\n",
        "metrics_dict={\n",
        "'Logistic Regression':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'XgBoost':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'Naive Bayes':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'Support Vector Machine':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'Decision Tree':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'Random Forest Classifier':{'accuracy':0,'precision':0,'recall':0,'f1':0},\n",
        "'Voting Classifier':{'accuracy':0,'precision':0,'recall':0,'f1':0}}\n",
        "\n",
        "metrics_df = pd.DataFrame.from_dict(metrics_dict,orient='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU_7VK7sQNBE"
      },
      "source": [
        "### Using Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZ389chtmve",
        "outputId": "ae5bb58b-e03c-492f-96e6-99f5f20c4f66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eviofekeze/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb Cell 65\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y120sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#classification\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y120sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m lg_clf \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y120sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m lg_clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y120sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# predict the labels on test dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eviofekeze/Documents/Data_Projects/FakeNews/Real_Fake_News.ipynb#Y120sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m predictions \u001b[39m=\u001b[39m lg_clf\u001b[39m.\u001b[39mpredict(X_test)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1138\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1139\u001b[0m     X,\n\u001b[1;32m   1140\u001b[0m     y,\n\u001b[1;32m   1141\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1142\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1143\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1144\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1145\u001b[0m )\n\u001b[1;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
            "File \u001b[0;32m~/anaconda3/envs/dsb/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "# X_ft = news_ft.drop(columns=['Real'])\n",
        "X_ft = news_ft[[          \n",
        "          'text_length',\n",
        "          # 'flesch_score',\n",
        "          'dale_chall_score',\n",
        "          # 'num_shared',\n",
        "          # 'avg_follower',\n",
        "          # 'avg_followee',\n",
        "          # 'closenness_centrality',\n",
        "          # 'betweenness_centrality',\n",
        "          # 'title_allcaps',\n",
        "          'title_allcaps_or_exmarks',\n",
        "\n",
        "          # 'title_num_exmarks',\n",
        "          'title_length',\n",
        "          'shared_by_top',\n",
        "          'title_comp', \n",
        "          'text_comp', \n",
        "          'title_stopwords', \n",
        "          'text_stopwords',\n",
        "          'f_ratio',\n",
        "          ]]\n",
        "y = news_ft['Real']\n",
        "count = CountVectorizer(stop_words='english')\n",
        "X_txt = count.fit_transform(news['text'])\n",
        "X_txt = X_txt.todense()\n",
        "\n",
        "X = np.hstack((X_ft,X_txt))\n",
        "X=X.astype('float')\n",
        "y = y.astype('int')\n",
        "\n",
        "try:\n",
        "    from sklearn.utils._testing import ignore_warnings\n",
        "except ImportError:\n",
        "    from sklearn.utils.testing import ignore_warnings\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "#X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "    X_train = X[tr_ind]\n",
        "    X_test = X[tst_ind]\n",
        "    y_train = y[tr_ind]\n",
        "    y_test = y[tst_ind]\n",
        "    #classification\n",
        "    lg_clf = LogisticRegression(max_iter=200)\n",
        "    lg_clf.fit(X_train, y_train)  \n",
        "    # predict the labels on test dataset\n",
        "    predictions = lg_clf.predict(X_test)\n",
        "\n",
        "    #evaluation\n",
        "    precision_list.append(metrics.precision_score(y_test, predictions))\n",
        "    recall_list.append(metrics.recall_score(y_test, predictions))\n",
        "    f1_list.append(metrics.f1_score(y_test, predictions))\n",
        "    accuracy_list.append(metrics.accuracy_score(y_test,predictions))\n",
        "    auc_scores.append(roc_auc_score(y_test, predictions))\n",
        "    AVG_precision_scores.append(average_precision_score(y_test, predictions))\n",
        "#     print(metrics.confusion_matrix(y_test,predictions))\n",
        "metrics_df.loc['Logistic Regression','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Logistic Regression','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Logistic Regression','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Logistic Regression','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4dhCANC2_sY"
      },
      "source": [
        "### XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThSlfP9stnDh",
        "outputId": "2e0d8e70-18ca-44c0-9348-35c2a93a36cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  78.466 \n",
            " recall     =  71.539 \n",
            " f1         =  74.517 \n",
            " accuracy   =  75.583 \n",
            " AUROC      =  78.782 \n",
            " Average Precision =  73.693 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#X= news_ft.drop(columns=['Real'])\n",
        "X_ft = news_ft[[          \n",
        "          'text_length',\n",
        "          'flesch_score',\n",
        "          'dale_chall_score',\n",
        "          # 'num_shared',\n",
        "          # 'avg_follower',\n",
        "          # 'avg_followee',\n",
        "          # 'closenness_centrality',\n",
        "          # 'betweenness_centrality',\n",
        "          # 'title_allcaps',\n",
        "          'title_allcaps_or_exmarks',\n",
        "          'title_num_exmarks',\n",
        "          'title_length',\n",
        "          'shared_by_top',\n",
        "          'title_comp', \n",
        "          'text_comp', \n",
        "          'title_stopwords', \n",
        "          'text_stopwords',\n",
        "          'f_ratio',\n",
        "          ]]\n",
        "y = news_ft['Real']\n",
        "# y = news['Real']\n",
        "\n",
        "count = CountVectorizer(stop_words='english')\n",
        "X_txt = count.fit_transform(news['text'])\n",
        "X_txt = X_txt.todense()\n",
        "\n",
        "X = np.hstack((X_ft,X_txt))\n",
        "X=X.astype('float')\n",
        "y = y.astype('int')\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "\n",
        "#X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "\n",
        "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
        "param['nthread'] = 4\n",
        "param['eval_metric'] = 'error'\n",
        "num_round = 10\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "    X_train = X[tr_ind,:]\n",
        "    X_test = X[tst_ind,:]\n",
        "    y_train = y[tr_ind]\n",
        "    y_test = y[tst_ind]\n",
        "    # evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
        "    #classification\n",
        "    xg_clf = xg.XGBClassifier(max_depth=10, learning_rate=0.5, n_estimators=8,\n",
        "                       objective='binary:logistic', booster='gbtree',min_child_weight=10)\n",
        "    xg_clf.fit(X_train, y_train)  \n",
        "    # predict the labels on test dataset\n",
        "    predictions = xg_clf.predict(X_test)\n",
        "\n",
        "    #evaluation\n",
        "    precision_list.append(metrics.precision_score(y_test, predictions))\n",
        "    recall_list.append(metrics.recall_score(y_test, predictions))\n",
        "    f1_list.append(metrics.f1_score(y_test, predictions))\n",
        "    accuracy_list.append(metrics.accuracy_score(y_test,predictions))\n",
        "    auc_scores.append(roc_auc_score(y_test, predictions))\n",
        "    AVG_precision_scores.append(average_precision_score(y_test, predictions))\n",
        "\n",
        "metrics_df.loc['XgBoost','precision'] =  round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['XgBoost','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['XgBoost','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['XgBoost','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e48avN81JxLu"
      },
      "source": [
        "### Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmBMmuAotWk6"
      },
      "outputs": [],
      "source": [
        "wordCount = CountVectorizer(stop_words='english')\n",
        "wordCount = wordCount.fit(news['text'])\n",
        "text_vector = wordCount.transform(news['text'])\n",
        "\n",
        "temp = text_vector.toarray()\n",
        "text_vector_df = pd.DataFrame(temp, columns=[f'TV{i}' for i in range(15428)], index=news.index)\n",
        "new_df = news_ft.join(text_vector_df, on = news_ft.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIs8_qeBdMKe",
        "outputId": "9202c677-80b6-4a7a-dfc4-35d63fec28f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  82.537 \n",
            " recall     =  84.341 \n",
            " f1         =  83.377 \n",
            " accuracy   =  83.174 \n",
            " AUROC      =  87.629 \n",
            " Average Precision =  86.864 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# X = text_vector_df.join(news_ft[['title_allcaps','dale_chall_score','title_num_exmarks','f_ratio','shared_by_top', \n",
        "#                         'title_allcaps_or_exmarks','title_comp', 'text_comp', 'title_stopwords', 'text_stopwords']], on=news_ft.index)\n",
        "X = text_vector_df.join(news_ft[['title_allcaps','dale_chall_score','title_num_exmarks','f_ratio','shared_by_top']], on=news_ft.index)\n",
        "\n",
        "y = new_df['Real'].astype('i4')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "  X_train = X.iloc[tr_ind]\n",
        "  X_test = X.iloc[tst_ind]\n",
        "  y_train = y[tr_ind]\n",
        "  y_test = y[tst_ind]\n",
        "\n",
        "  mn_clf = MultinomialNB()\n",
        "  mn_clf.fit(X_train, y_train) \n",
        "  predictions = mn_clf.predict(X_test)\n",
        "  proba = mn_clf.predict_proba(X_test)[:,1]\n",
        "      \n",
        "\n",
        "    #evaluation\n",
        "  precision = metrics.precision_score(y_test, predictions)\n",
        "  recall = metrics.recall_score(y_test, predictions)\n",
        "  f1 = metrics.f1_score(y_test, predictions)\n",
        "  accuracy = metrics.accuracy_score(y_test,predictions)\n",
        "      # print(metrics.confusion_matrix(y_test,predictions))\n",
        "      \n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  f1_list.append(f1)\n",
        "  accuracy_list.append(accuracy)\n",
        "  auc_scores.append(roc_auc_score(y_test, proba))\n",
        "  AVG_precision_scores.append(average_precision_score(y_test, proba))\n",
        "\n",
        "metrics_df.loc['Naive Bayes','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Naive Bayes','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Naive Bayes','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Naive Bayes','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIdRpo8fInRh"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaIu4NaErpD4"
      },
      "outputs": [],
      "source": [
        "wordCount = TfidfVectorizer(stop_words='english')\n",
        "wordCount = wordCount.fit(news['text'])\n",
        "text_vector = wordCount.transform(news['text'])\n",
        "\n",
        "temp = text_vector.toarray()\n",
        "text_vector_df = pd.DataFrame(temp, columns=[f'TV{i}' for i in range(15428)], index=news.index)\n",
        "new_df = news_ft.join(text_vector_df, on = news_ft.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T684m_EhIrPv",
        "outputId": "28961095-5a30-49b5-d027-b017b0e2e5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  74.366 \n",
            " recall     =  78.195 \n",
            " f1         =  76.157 \n",
            " accuracy   =  75.591 \n",
            " AUROC      =  85.562 \n",
            " Average Precision =  86.209 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "X = text_vector_df.join(news_ft[['title_allcaps','dale_chall_score','title_num_exmarks','f_ratio','shared_by_top',\n",
        "          'title_comp', 'text_comp', 'title_stopwords', 'text_stopwords']], on=news_ft.index)\n",
        "y = new_df['Real'].astype('i4')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "  X_train = X.iloc[tr_ind]\n",
        "  X_test = X.iloc[tst_ind]\n",
        "  y_train = y[tr_ind]\n",
        "  y_test = y[tst_ind]\n",
        "\n",
        "  sv_clf = svm.SVC(kernel= 'rbf', class_weight='balanced',probability=True, decision_function_shape='ovo',C=85)\n",
        "  sv_clf.fit(X_train, y_train) \n",
        "  predictions = sv_clf.predict(X_test)\n",
        "  proba = sv_clf.predict_proba(X_test)[:,1]\n",
        "      \n",
        "\n",
        "    #evaluation\n",
        "  precision = metrics.precision_score(y_test, predictions)\n",
        "  recall = metrics.recall_score(y_test, predictions)\n",
        "  f1 = metrics.f1_score(y_test, predictions)\n",
        "  accuracy = metrics.accuracy_score(y_test,predictions)\n",
        "      # print(metrics.confusion_matrix(y_test,predictions))\n",
        "      \n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  f1_list.append(f1)\n",
        "  accuracy_list.append(accuracy)\n",
        "  auc_scores.append(roc_auc_score(y_test, proba))\n",
        "  AVG_precision_scores.append(average_precision_score(y_test, proba))\n",
        "\n",
        "metrics_df.loc['Support Vector Machine','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Support Vector Machine','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Support Vector Machine','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Support Vector Machine','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvejxVtifgfn"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cisVMj2nfp5h",
        "outputId": "5f4e57ec-5a00-4312-c9e1-840fbe92ef20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  84.171 \n",
            " recall     =  79.081 \n",
            " f1         =  80.93 \n",
            " accuracy   =  81.504 \n",
            " AUROC      =  81.512 \n",
            " Average Precision =  76.854 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "wordCount = CountVectorizer(stop_words='english')\n",
        "wordCount = wordCount.fit(news['text'])\n",
        "text_vector = wordCount.transform(news['text'])\n",
        "tnew = news.copy()\n",
        "\n",
        "temp = text_vector.toarray()\n",
        "text_vector_df = pd.DataFrame(temp, columns=[f'TV{i}' for i in range(15428)], index=news.index)\n",
        "new_df = news_ft.join(text_vector_df, on = news_ft.index)\n",
        "X = text_vector_df.join(news_ft[[          \n",
        "          'text_length',\n",
        "          # 'flesch_score',\n",
        "          'dale_chall_score',\n",
        "          # 'num_shared',\n",
        "          # 'avg_follower',\n",
        "          # 'avg_followee',\n",
        "          'closenness_centrality',\n",
        "          # 'betweenness_centrality',\n",
        "          # 'title_allcaps',\n",
        "          'title_allcaps_or_exmarks',\n",
        "          # 'title_num_exmarks',\n",
        "          # 'title_length',\n",
        "          'shared_by_top',\n",
        "          'title_comp', \n",
        "          'text_comp', \n",
        "          # 'title_stopwords', \n",
        "          # 'text_stopwords',\n",
        "          # 'f_ratio',\n",
        "          ]], on=news_ft.index)\n",
        "y = tnew['Real'].astype('i4')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "  X_train = X.iloc[tr_ind]\n",
        "  X_test = X.iloc[tst_ind]\n",
        "  y_train = y[tr_ind]\n",
        "  y_test = y[tst_ind]\n",
        "\n",
        "  tr_clf = DecisionTreeClassifier( )\n",
        "  tr_clf = tr_clf.fit(X_train, y_train)\n",
        "  predictions = tr_clf.predict(X_test)\n",
        "  proba = tr_clf.predict_proba(X_test)[:,1]\n",
        "      \n",
        "\n",
        "    #evaluation\n",
        "  precision = metrics.precision_score(y_test, predictions)\n",
        "  recall = metrics.recall_score(y_test, predictions)\n",
        "  f1 = metrics.f1_score(y_test, predictions)\n",
        "  accuracy = metrics.accuracy_score(y_test,predictions)\n",
        "      # print(metrics.confusion_matrix(y_test,predictions))\n",
        "      \n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  f1_list.append(f1)\n",
        "  accuracy_list.append(accuracy)\n",
        "  auc_scores.append(roc_auc_score(y_test, proba))\n",
        "  AVG_precision_scores.append(average_precision_score(y_test, proba))\n",
        "\n",
        "metrics_df.loc['Decision Tree','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Decision Tree','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Decision Tree','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Decision Tree','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmH5FQItgEcK"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xs0-zHFgMge",
        "outputId": "b73979c7-e47b-4e6f-ee91-3e5f7dccdbd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  83.763 \n",
            " recall     =  76.268 \n",
            " f1         =  79.649 \n",
            " accuracy   =  80.56 \n",
            " AUROC      =  90.028 \n",
            " Average Precision =  91.301 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "X = text_vector_df.join(news_ft[[          \n",
        "          # 'title_allcaps',\n",
        "          'title_allcaps_or_exmarks',\n",
        "          # 'title_num_exmarks',\n",
        "\n",
        "          'title_length',\n",
        "          # 'text_length',\n",
        "\n",
        "          # 'flesch_score',\n",
        "\n",
        "          'dale_chall_score',\n",
        "          # 'num_shared',\n",
        "\n",
        "          'shared_by_top',\n",
        "          'avg_follower',\n",
        "          'title_comp', \n",
        "\n",
        "          'text_comp', \n",
        "          'title_stopwords', \n",
        "          # 'text_stopwords',\n",
        "          # 'avg_followee',\n",
        "          'f_ratio',\n",
        "          # 'closenness_centrality',\n",
        "          # 'betweenness_centrality',\n",
        "          ]], on=news_ft.index)\n",
        "y = tnew['Real'].astype('i4')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "  X_train = X.iloc[tr_ind]\n",
        "  X_test = X.iloc[tst_ind]\n",
        "  y_train = y[tr_ind]\n",
        "  y_test = y[tst_ind]\n",
        "\n",
        "  rnd_clf = RandomForestClassifier(n_estimators= 1000)\n",
        "  rnd_clf = rnd_clf.fit(X_train, y_train)\n",
        "  predictions = rnd_clf.predict(X_test)\n",
        "  proba = rnd_clf.predict_proba(X_test)[:,1]\n",
        "      \n",
        "\n",
        "    #evaluation\n",
        "  precision = metrics.precision_score(y_test, predictions)\n",
        "  recall = metrics.recall_score(y_test, predictions)\n",
        "  f1 = metrics.f1_score(y_test, predictions)\n",
        "  accuracy = metrics.accuracy_score(y_test,predictions)\n",
        "      # print(metrics.confusion_matrix(y_test,predictions))\n",
        "      \n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  f1_list.append(f1)\n",
        "  accuracy_list.append(accuracy)\n",
        "  auc_scores.append(roc_auc_score(y_test, proba))\n",
        "  AVG_precision_scores.append(average_precision_score(y_test, proba))\n",
        "\n",
        "metrics_df.loc['Random Forest Classifier','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Random Forest Classifier','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Random Forest Classifier','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Random Forest Classifier','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUCVAprTeJzU"
      },
      "source": [
        "### Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv5EJW1h-4QG",
        "outputId": "73194d20-59dc-4fa8-ae11-e42a814f9ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " precision  =  85.164 \n",
            " recall     =  83.389 \n",
            " f1         =  84.216 \n",
            " accuracy   =  84.359 \n",
            " AUROC      =  91.907 \n",
            " Average Precision =  92.154 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "X = text_vector_df.join(news_ft[['title_allcaps','title_isascii','dale_chall_score','title_num_exmarks','f_ratio','shared_by_top',\n",
        "                      'title_comp', 'text_comp', 'title_stopwords', 'text_stopwords']], on=news_ft.index)\n",
        "y = new_df['Real'].astype('i4')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "precision_list=[]\n",
        "recall_list=[]\n",
        "f1_list=[]\n",
        "accuracy_list=[]\n",
        "auc_scores = []\n",
        "AVG_precision_scores = []\n",
        "\n",
        "\n",
        "for tr_ind, tst_ind in skf.split(X,y):\n",
        "  X_train = X.iloc[tr_ind]\n",
        "  X_test = X.iloc[tst_ind]\n",
        "  y_train = y[tr_ind]\n",
        "  y_test = y[tst_ind]\n",
        "\n",
        "  # clf = svm.SVC(kernel= 'rbf', class_weight='balanced',probability=True, decision_function_shape='ovo',C=85)\n",
        "  # clf.fit(X_train, y_train) \n",
        "\n",
        "  log_clf_ = LogisticRegression(max_iter=200)\n",
        "  NV_clf_ = MultinomialNB()\n",
        "  svm_clf_ = svm.SVC(kernel= 'rbf', class_weight='balanced',probability=True, decision_function_shape='ovo',C=85)\n",
        "  tr_clf_ = RandomForestClassifier(n_estimators=1000)\n",
        "  xg_clf_ = xg.XGBClassifier(max_depth=10, learning_rate=0.5, n_estimators=8,\n",
        "                       objective='binary:logistic', booster='gbtree',min_child_weight=10)\n",
        "\n",
        "\n",
        "  vtg_clf = VotingClassifier(\n",
        "      estimators=[('lr', log_clf_), ('NV', NV_clf_),('forest', tr_clf_)],voting='soft')\n",
        "\n",
        "  vtg_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "  predictions = vtg_clf.predict(X_test)\n",
        "  proba = vtg_clf.predict_proba(X_test)[:,1]\n",
        "      \n",
        "\n",
        "    #evaluation\n",
        "  precision = metrics.precision_score(y_test, predictions)\n",
        "  recall = metrics.recall_score(y_test, predictions)\n",
        "  f1 = metrics.f1_score(y_test, predictions)\n",
        "  accuracy = metrics.accuracy_score(y_test,predictions)\n",
        "      # print(metrics.confusion_matrix(y_test,predictions))\n",
        "      \n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  f1_list.append(f1)\n",
        "  accuracy_list.append(accuracy)\n",
        "  auc_scores.append(roc_auc_score(y_test, proba))\n",
        "  AVG_precision_scores.append(average_precision_score(y_test, proba))\n",
        "\n",
        "\n",
        "metrics_df.loc['Voting Classifier','precision'] = round(np.mean(precision_list)*100,3)\n",
        "metrics_df.loc['Voting Classifier','recall'] = round(np.mean(recall_list)*100,3)\n",
        "metrics_df.loc['Voting Classifier','f1'] = round(np.mean(f1_list)*100,3)\n",
        "metrics_df.loc['Voting Classifier','accuracy'] = round(np.mean(accuracy_list)*100,3)\n",
        "\n",
        "print(\" precision  = \", round(np.mean(precision_list)*100,3),\"\\n\", \n",
        "      \"recall     = \",round(np.mean(recall_list)*100,3),\"\\n\",\n",
        "      \"f1         = \",round(np.mean(f1_list)*100,3),\"\\n\",\n",
        "      \"accuracy   = \",round(np.mean(accuracy_list)*100,3),\"\\n\", \n",
        "      \"AUROC      = \", round(np.mean(auc_scores)*100,3),\"\\n\",\n",
        "      \"Average Precision = \", round(np.mean(AVG_precision_scores)*100,3),\"\\n\", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkYiCSXd1eSb"
      },
      "source": [
        "## Summary of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "HOb_huJihKvW",
        "outputId": "f2b716bd-b09f-4f72-811b-8ecb53375c4a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <td>81.989</td>\n",
              "      <td>84.326</td>\n",
              "      <td>78.627</td>\n",
              "      <td>81.140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XgBoost</th>\n",
              "      <td>75.583</td>\n",
              "      <td>78.466</td>\n",
              "      <td>71.539</td>\n",
              "      <td>74.517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Naive Bayes</th>\n",
              "      <td>83.174</td>\n",
              "      <td>82.537</td>\n",
              "      <td>84.341</td>\n",
              "      <td>83.377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Support Vector Machine</th>\n",
              "      <td>75.591</td>\n",
              "      <td>74.366</td>\n",
              "      <td>78.195</td>\n",
              "      <td>76.157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree</th>\n",
              "      <td>81.504</td>\n",
              "      <td>84.171</td>\n",
              "      <td>79.081</td>\n",
              "      <td>80.930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest Classifier</th>\n",
              "      <td>80.560</td>\n",
              "      <td>83.763</td>\n",
              "      <td>76.268</td>\n",
              "      <td>79.649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Voting Classifier</th>\n",
              "      <td>84.359</td>\n",
              "      <td>85.164</td>\n",
              "      <td>83.389</td>\n",
              "      <td>84.216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          accuracy  precision  recall      f1\n",
              "Logistic Regression         81.989     84.326  78.627  81.140\n",
              "XgBoost                     75.583     78.466  71.539  74.517\n",
              "Naive Bayes                 83.174     82.537  84.341  83.377\n",
              "Support Vector Machine      75.591     74.366  78.195  76.157\n",
              "Decision Tree               81.504     84.171  79.081  80.930\n",
              "Random Forest Classifier    80.560     83.763  76.268  79.649\n",
              "Voting Classifier           84.359     85.164  83.389  84.216"
            ]
          },
          "execution_count": 42,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Nfuk1xmF1t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('dsb')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "8bee33690903381dd105b525ae0551d31ce430bb6c8585d7109958867dd375a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
